Basic-Example
=============
---
  kind: Pod                          # Object Type
  apiVersion: v1                     # API version
  metadata:                          # Set of data which describes the Object
    name: samplepod                  # Name of the Object
  spec:                              # Data which describes the state of the Object
    containers:                      # Data which describes the Container details
      - name: samplecont             # Name of the Container
        image: ubuntu:latest         # Base Image which is used to create Container
        command: ["/bin/bash", "-c", "while true; do echo Hello-Kubernetes; sleep 5 ; done"]
    restartPolicy: Never             # Defaults to Always


command
=======
kubectl apply -f testpod.yaml
kubectl get pods
kubectl get pods -o wide
kubectl describe pod samplepod
kubectl exec samplepod -it /bin/bash
kubectl exec samplepod -it -c samplecont /bin/bash
kubectl exec samplepod -- hostname -i
kubectl exec samplepod date
kubectl exec samplepod -c samplecont date
kubectl exec samplepod -c samplecont ls
kubectl logs -f samplepod
kubectl logs -f samplepod -c samplecont
kubectl delete -f testpod.yaml



=======================

---
  kind: Pod
  apiVersion: v1
  metadata:
    name: environments
  spec:
    containers:
      - name: mycont
        image: ubuntu:latest
        command: ["/bin/bash", "-c", "while true; do echo Hello-kubernetes; sleep 5 ; done"]
        env:                        # List of environment variables to be used inside the pod
          - name: MYNAME
            value: PPNREDDY

commands
========
kubectl exec environments -- /bin/bash -c  'echo $MYNAME'

==================
---
  kind: Pod
  apiVersion: v1
  metadata:
    name: http-pod
  spec:
    containers:
      - name: mycont
        image: httpd:latest
        ports:
          - containerPort: 80

  Commands
  kubectl exec http-pod -- hostname -i
  curl 10.244.54.7:80
  <html><body><h1>It works!</h1></body></html>


 
=====================

Pods with labels
================

---
  kind: Pod
  apiVersion: v1
  metadata:
    name: labelspod
    labels:                       # Specifies the Label details under it
      env: development
      class: pods
  spec:
    containers:
      - name: labelcont
        image: ubuntu:latest
        command: ["/bin/bash", "-c", "while true; do echo Hello-kubernetes; sleep 5 ; done"]

Commands
========

kubectl get pods --show-labels
kubectl label pods <podname> <labelkey>=<value>
kubectl get pods -l env=development
kubectl get pods -l env=development,class=pods
kubectl delete pod -l <labelkey>=<value>
kubectl get pods -l 'env in (development,testing)'
kubectl get pods -l 'env notin (development,testing)'

===============

Pod with annotations
====================
---
  kind: Pod
  apiVersion: v1
  metadata:
    name: annotations-pod
    labels:
      app: apache
    annotations:
      description: "Deployment for Apache HTTP Server"
      maintainer: "ppreddy@hcltech.com"
  spec:
    containers:
      - name: mycont
        image: httpd:latest
        ports:
          - containerPort: 80


===================

Multi container pods
====================

kind: Pod
apiVersion: v1
metadata:
  name: multicontpod
spec:
  containers:
    - name: container-1
      image: ubuntu:latest
      command: ["/bin/bash", "-c", "while true; do echo Hello-India; sleep 5 ; done"]
    - name: container-2
      image: ubuntu:latest
      command: ["/bin/bash", "-c", "while true; do echo Hello-Karnataka; sleep 5 ; done"]

Commands
========
kubectl get pods
kubectl exec multicontpod -it -c container-1 /bin/bash
kubectl exec multicontpod -it -c container-2 /bin/bash
kubectl logs -f multicontpod -c container-1
kubectl logs -f multicontpod -c container-2

========================

Node Selector
=============
kubectl label nodes ip-10-1-1-18 disktype=ssd

---
  apiVersion: v1
  kind: Pod
  metadata:
    name: my-pod
  spec:
    containers:
      - name: container-1
        image: ubuntu:latest
        command: ["/bin/bash", "-c", "while true; do echo Hello-India; sleep 5 ; done"]
    nodeSelector:
      disktype: ssd

Commands:
kubectl apply -f testpod.yaml
kubectl get pods -o wide
kubectl describe node ip-10-1-1-18

=============================================
---
  kind: ReplicationController         # This defines to create the object of replication type
  apiVersion: v1
  metadata:
    name: myrc
  spec:
    replicas: 2       # This element defines the desired number of pods
    selector: # Tells the controller which pods to watch/belong to this Replication Controller
      myname: ppreddy   # These must match the labels
    template:           # Template element defines a template to launch a new pod
      metadata:
        name: rcpod
        labels:            
          myname: ppreddy
      spec:
        containers:
          - name: rccontainer
            image: ubuntu:latest
            command: ["/bin/bash", "-c", "while true; do echo Hello-ppreddy; sleep 5 ; done"]

Commands
========

kubectl apply -f rc-demo.yaml
kubectl get rc
kubectl get pods
kubectl describe rc myrc
kubectl get pods -o wide
kubectl delete pod myrc-pt9d6
kubectl get pods -o wide

kubectl get pods -l myname=ppreddy
kubectl scale rc myrc --replicas=10
kubectl scale rc myrc --replicas=5

kubectl delete -f rc-demo.yaml


======================================

## ReplicaSet Demo

---
  kind: ReplicaSet                        # Defines the object to be ReplicaSet
  apiVersion: apps/v1                     # Replicaset is not available on v1
  metadata:
    name: myrs
  spec:
    replicas: 2  
    selector:                  
      matchExpressions:                    # These must match the labels
        - {key: myname, operator: In, values: [ppreddy, ppnreddy, preddy]}
        - {key: env, operator: NotIn, values: [production]}
    template:      
      metadata:
        name: rspod
        labels:              
          myname: ppreddy
          env: dev
      spec:
        containers:
          - name: rscontainer
            image: ubuntu:latest
            command: ["/bin/bash", "-c", "while true; do echo Hello-ppreddy; sleep 5 ; done"]



Commands
========
kubectl apply -f rs-demo.yaml
kubectl get rs
kubectl describe rs myrs

kubectl get pods
kubectl delete pod myrs-v7ggf
kubectl get pods

kubectl get pods -l myname=ppreddy
kubectl get pods -l env=dev

kubectl scale rs myrs --replicas=10
kubectl scale rs myrs --replicas=5

kubectl delete -f rs-demo.yaml


=============================================
---
  kind: Deployment
  apiVersion: apps/v1
  metadata:
    name: mydeployments
  spec:
    replicas: 2
    selector:      # tells the controller which pods to watch/belong to
      matchLabels:
        name: deployment
    template:
      metadata:
        name: deppod
        labels:
          name: deployment
      spec:
        containers:
          - name: dep-cont
            image: ubuntu:latest
            command: ["/bin/bash", "-c", "while true; do echo Hello-DevOps; sleep 5; done"]

Commands
========
kubectl apply -f deploy-demo.yaml
kubectl get deployment
kubectl describe deployment mydeployments
kubectl get rs
kubectl get pods
kubectl get pods -o wide

kubectl delete pod  mydeployments-84877d89c9-vw9gd
kubectl get pods

kubectl delete rs mydeployments-84877d89c9
kubectl get rs
kubectl get pods

kubectl scale deployment mydeployments --replicas=10
kubectl get deployment
kubectl describe deployment mydeployments
kubectl get rs
kubectl get pods

kubectl scale deployment mydeployments --replicas=1
kubectl get deployment
kubectl describe deployment mydeployments
kubectl get rs
kubectl get pods

kubectl logs -f mydeployments-84877d89c9-7pk2m

Lets deploy another version
---------------------------
---
  kind: Deployment
  apiVersion: apps/v1
  metadata:
    name: mydeployments
  spec:
    replicas: 2
    selector:      # tells the controller which pods to watch/belong to
      matchLabels:
        name: deployment
    template:
      metadata:
        name: deppod
        labels:
          name: deployment
      spec:
        containers:
          - name: dep-cont
            image: centos:latest
            command: ["/bin/bash", "-c", "while true; do echo Hello-DevOps-centos; sleep 5; done"]


Commands
========
kubectl apply -f deploy-demo.yaml
kubectl get deployment
kubectl describe deployment mydeployments
kubectl get rs
kubectl get pods
kubectl get pods -o wide

kubectl logs -f mydeployments-9cf4bc565-tn9z2

kubectl rollout status deployment mydeployments
kubectl rollout history deployment mydeployments
kubectl rollout undo deploy/mydeployments --to-revision=1

kubectl get rs
kubectl get pods
kubectl get pods -o wide

kubectl logs -f mydeployments-84877d89c9-dwm26
kubectl get deployment
kubectl describe deployment mydeployments

kubectl delete -f deploy-demo.yaml


==========================

# DaemonSet demo

---
  apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    name: nginx-ds
  spec:
    selector:
      matchLabels:
        app: nginx-app
    template:
      metadata:
        name: nginx-pod
        labels:
          app: nginx-app
      spec:
        containers:
          - name: nginx-container
            image: nginx:latest
            ports:
              - containerPort: 80

Commands
========
kubectl apply -f ds-demo.yaml

kubectl get ds
kubectl get rs
kubectl get pods
kubectl get pods -o wide
kubectl describe ds nginx-ds

kubectl delete -f  ds-demo.yaml


===================================
Container to Container communication
====================================

ctoc.yaml
=========

---
  kind: Pod
  apiVersion: v1
  metadata:
    name: testpod
  spec:
    containers:
      - name: uc
        image: ubuntu:latest
        command: ["/bin/bash", "-c", "while true; do echo Hello-Kubernetes; sleep 5 ; done"]
      - name: hc
        image: httpd:latest
        ports:
         - containerPort: 80

commands:
=========
kubectl apply -f ctoc.yaml
kubectl exec testpod -it -c uc -- /bin/bash
root@testpod:/# apt update && apt install curl
root@testpod:/# curl localhost:80
kubectl delete -f ctoc.yaml



============================
Pod to Pod communication
========================

podnginx.yaml
=============

---
  kind: Pod
  apiVersion: v1
  metadata:
    name: testpodnginx
  spec:
    containers:
      - name: nc
        image: nginx:latest
        ports:
         - containerPort: 80


podhttpd.yaml
=============
---
  kind: Pod
  apiVersion: v1
  metadata:
    name: testpodhttpd
  spec:
    containers:
      - name: hc
        image: httpd:latest
        ports:
         - containerPort: 80

commands:
=========
kubectl apply -f podnginx.yaml
kubectl apply -f podhttpd.yaml
kubectl get pods
kubectl get pods -o wide
curl 10.244.54.42:80
curl 10.244.54.41:80
kubectl delete -f podnginx.yaml
kubectl delete -f podhttpd.yaml


=====================================================
clusterIP-Service

clusteripdeployment.yaml
========================

---
  kind: Deployment
  apiVersion: apps/v1
  metadata:
    name: mydeployments
  spec:
    replicas: 2
    selector:      # tells the controller which pods to watch/belong to
      matchLabels:
        name: deployment
    template:
      metadata:
        name: testpod8
        labels:
          name: deployment
      spec:
        containers:
          - name: hc
            image: httpd
            ports:
              - containerPort: 80



clusteripservice.yaml
=====================

---
  kind: Service                  # Defines to create Service type Object
  apiVersion: v1
  metadata:
    name: democipservice
  spec:
    ports:
      - port: 80                # Containers port exposed
        targetPort: 80          # Pods port
    selector:
      name: deployment          # Apply this service to any pods which has the specific label
    type: ClusterIP             # Specifies the service type i.e ClusterIP or NodePort


commands:
=========
kubectl apply -f clusteripdeployment.yaml
kubectl apply -f clusteripservice.yaml
kubectl get pods
kubectl get svc
curl 10.96.78.101:80
kubectl delete pod mydeployments-779f4dbf96-rc89d
kubectl get pods
curl 10.96.78.101:80


================================
NodePort Service
=================


nodeportdeployment.yaml
========================

---
  kind: Deployment
  apiVersion: apps/v1
  metadata:
    name: mydeployments
  spec:
    replicas: 4
    selector:      # tells the controller which pods to watch/belong to
      matchLabels:
        name: deployment
    template:
      metadata:
        name: testpod8
        labels:
          name: deployment
      spec:
        containers:
          - name: hc
            image: httpd
            ports:
              - containerPort: 80



nodeportservice.yaml
====================

---
  kind: Service                  # Defines to create Service type Object
  apiVersion: v1
  metadata:
    name: demonpservice
  spec:
    ports:
      - port: 80                # Containers port exposed
        targetPort: 80          # Pods port
    selector:
      name: deployment          # Apply this service to any pods which has the specific label
    type: NodePort             # Specifies the service type i.e ClusterIP or NodePort


commands:
=========
kubectl apply -f nodeportdeployment.yaml
kubectl apply -f nodeportservice.yaml
kubectl get pods
kubectl get svc

access URL's in browser:
   http://18.218.106.172:30647/
   http://3.145.94.69:30647/
   http://3.129.253.12:30647/

kubectl delete -f nodeportdeployment.yaml
kubectl delete -f nodeportservice.yaml


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

---
  kind: Deployment
  apiVersion: apps/v1
  metadata:
    name: mydeployments
  spec:
    replicas: 1
    selector:      # tells the controller which pods to watch/belong to
      matchLabels:
        name: deployment
    template:
      metadata:
        name: testpod8
        labels:
          name: deployment
      spec:
        containers:
          - name: hc
            image: httpd
            ports:
              - containerPort: 80


apiVersion: v1
kind: Service
metadata:
  name: my-app-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"  # or "clb" for Classic Load Balancer
spec:
  selector:
    app: deployment
  ports:
    - protocol: TCP
      port: 80            # Expose on port 80
      targetPort: 8080     # Target container port
  type: LoadBalancer      # Create an ELB


  ===========================

  Volumes
=======

emptyDir
--------


kubectl apply -f empty-dir-pods.yml
===================================

---
  apiVersion: v1
  kind: Pod
  metadata:
    name: myvolemptydir
  spec:
    containers:
     - name: uc1
       image: ubuntu:latest
       command: ["/bin/bash", "-c", "sleep 10000"]
       volumeMounts:                         # Mount definition inside the container
        - name: xchange
          mountPath: "/tmp/xchange"          # Path inside the container to share
     - name: uc2
       image: ubuntu:latest
       command: ["/bin/bash", "-c", "sleep 10000"]
       volumeMounts:
        - name: xchange
          mountPath: "/tmp/data"
    volumes:                                 # Definition for host
     - name: xchange
       emptyDir: {}


Commands:
=========

kubectl apply -f empty-dir-pods.yml
kubectl exec myvolemptydir -c uc1 -it -- /bin/bash
cd /tmp/xchange
touch uc1.txt

kubectl exec myvolemptydir -c uc2 -it -- /bin/bash
cd /tmp/data/
ls
touch uc2.txt
kubectl delete -f empty-dir-pods.yml



hostPath
--------

hostpath-pods.yml
=================
---
  apiVersion: v1
  kind: Pod
  metadata:
    name: myhostpath
  spec:
    containers:
     - name: uc1
       image: ubuntu:latest
       command: ["/bin/bash", "-c", "sleep 10000"]
       volumeMounts:                         # Mount definition inside the container
        - mountPath: /tmp/hostpath
          name: testvolume
    volumes:                                 # Definition for host
     - name: testvolume
       hostPath:
         path: /tmp/data
         type: DirectoryOrCreate

Commands:
=========
kubectl apply -f hostpath-pods.yml
kubectl get pods -o wide

verify the host where it's created
kubectl delete -f hostpath-pods.yml

=====================


Static provisioning
===================

Create EBS volume in same az and copy the volume id: vol-0c6bcdb099f8bad56

mypv.yaml
---------
apiVersion: v1
kind: PersistentVolume
metadata:
  name: myebsvol
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce 
  persistentVolumeReclaimPolicy: Recycle
  awsElasticBlockStore:
    volumeID: vol-09d341ec00318bab7
    fsType: ext4

#ReadWriteOnce means that the volume can be mounted as read-write by a single node at a time.

#The reclaim policy specifies what happens to the volume when it is released by its associated PersistentVolumeClaim (PVC).

#Recycle means that when the PVC is deleted, the volume will be wiped (its data deleted) and it can be reused by another claim.

#Retain: You want to preserve the data for manual retrieval or reuse.
#Recycle: You want the volume to be cleared of data and available for reuse (though deprecated).
#Delete: You want to completely remove both the volume and the data when no longer needed.

Access Mode Description
=======================
RWO:  ReadWriteOnce Volume can be mounted as read-write by a single node. 
ROX:  ReadOnlyMany  Volume can be mounted as read-only by multiple nodes. 
RWX:  ReadWriteMany Volume can be mounted as read-write by multiple nodes.  
RWOP: ReadWriteOncePod  Volume can be mounted as read-write by a single pod (strictest).



mypvc.yaml
----------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myebsvolclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi





staticvolumepod.yaml
--------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pvdeploy
spec:
  replicas: 1
  selector:      # tells the controller which pods to watch/belong to
    matchLabels:
     app: mypv
  template:
    metadata:
      labels:
        app: mypv
    spec:
      containers:
      - name: shell
        image: centos
        command: ["bin/bash", "-c", "sleep 10000"]
        volumeMounts:
        - name: mypd
          mountPath: "/tmp/persistent"
      volumes:
        - name: mypd
          persistentVolumeClaim:
            claimName: myebsvolclaim



Commands:
=========
kubectl apply -f mypv.yaml
kubectl get pv

kubectl apply -f mypvc.yaml
kubectl get pvc

kubectl apply -f staticvolumepod.yaml


kubectl delete -f mypv.yaml
kubectl delete -f mypvc.yaml
kubectl delete -f staticvolumepod.yaml


===============================

liveness-probe.yaml
===================
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: mylivenessprobe
spec:
  containers:
  - name: liveness
    image: ubuntu
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 1000
    livenessProbe:                             # define the health check
      exec:
        command:                               # command to run periodically
        - cat                
        - /tmp/healthy
      initialDelaySeconds: 5      # Wait for the specified time before it runs the first probe
      periodSeconds: 5            # Run the above command every 5 sec
      timeoutSeconds: 30          # Command must finish within 30 seconds                   

Commands
========
kubectl apply -f liveness-probe.yaml
kubectl exec -it mylivenessprobe -- /bin/bash

cat /tmp/healthy
echo $?
rm /tmp/healthy

kubectl describe pod mylivenessprobe
kubectl delete -f liveness-probe.yaml



===============================
Readinessprobe-demo
==================

rpdeployment.yaml
=================

---
  kind: Deployment
  apiVersion: apps/v1
  metadata:
    name: mydeployments
  spec:
    replicas: 3
    selector:      # tells the controller which pods to watch/belong to
      matchLabels:
        name: deployment
    template:
      metadata:
        name: testpod8
        labels:
          name: deployment
      spec:
        containers:
          - name: hc
            image: httpd
            ports:
              - containerPort: 80
            args:
              - /bin/sh
              - -c
              - touch /tmp/healthy; sleep 1000
            readinessProbe:
              exec:
               command:
                 - cat
                 - /tmp/ready 
              initialDelaySeconds: 15     # Wait 15 seconds before performing the first check
              periodSeconds: 10          # Check every 10 seconds
              timeoutSeconds: 30          # Command must finish within 30 seconds
              successThreshold: 1    # A single successful check marks the container as ready
              failureThreshold: 3    # 3 consecutive failures will mark the pod as not ready


clusteripservice.yaml
=====================

---
  kind: Service                  # Defines to create Service type Object
  apiVersion: v1
  metadata:
    name: democipservice
  spec:
    ports:
      - port: 80                # Containers port exposed
        targetPort: 80          # Pods port
    selector:
      name: deployment          # Apply this service to any pods which has the specific label
    type: ClusterIP             # Specifies the service type i.e ClusterIP or NodePort


commands:
=========
kubectl apply -f clusteripdeployment.yaml
kubectl apply -f clusteripservice.yaml
kubectl get pods
kubectl get svc
curl 10.96.78.101:80
kubectl delete pod mydeployments-779f4dbf96-rc89d
kubectl get pods
curl 10.96.78.101:80

kubectl delete -f clusteripdeployment.yaml
kubectl delete -f clusteripservice.yaml


======================================
config maps demo
================


configmap.yaml
==============

apiVersion: v1
kind: ConfigMap
metadata:
  name: example-config
data:
  # Property-like keys
  config.properties: |
    setting1=psd
    setting2=devops
  # Single key-value pair
  log_level: "INFO"
  # JSON file
  data.json: |
    {
      "key1": "aws",
      "key2": "kubernetes"
    }



configmap-env.yaml
==================

apiVersion: v1
kind: Pod
metadata:
  name: app-pod-env
spec:
  containers:
  - name: app-env-container
    image: nginx
    env:
    - name: LOG_LEVEL
      valueFrom:
        configMapKeyRef:
          name: example-config
          key: log_level


configmap-vol.yaml
==================

apiVersion: v1
kind: Pod
metadata:
  name: app-pod-vol
spec:
  containers:
  - name: app-vol-container
    image: nginx
    volumeMounts:
    - name: config-volume
      mountPath: /etc/config
  volumes:
  - name: config-volume
    configMap:
      name: example-config

commands:
=========
kubectl apply -f configmap.yaml
kubectl apply -f configmap-env.yaml
kubectl apply -f configmap-vol.yaml

kubectl get configmaps
kubectl describe configmap example-config

kubectl exec -it  app-pod-env -- /bin/bash
echo $LOG_LEVEL

kubectl exec -it  app-pod-vol -- /bin/bash
cd /etc/config/

cat config.properties
cat data.json
cat log_level


kubectl delete -f configmap.yaml
kubectl delete -f configmap-env.yaml
kubectl delete -f configmap-vol.yaml


secrets  demo
=============

Create a Secret
===============

kubectl create secret generic my-secret \
  --from-literal=username=ppreddy \
  --from-literal=password=India@123

kubectl get secrets my-secret -o yaml


echo -n 'ppreddy' | base64

echo -n 'India@123' | base64

ubuntu@ip-10-1-1-187:~$ echo -n 'ppreddy' | base64
cHByZWRkeQ==
ubuntu@ip-10-1-1-187:~$ echo -n 'India@1234' | base64
SW5kaWFAMTIzNA==
ubuntu@ip-10-1-1-187:~$


secret.yaml
===========
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  username: cHByZWRkeQ==   # 'myuser' base64-encoded
  password: SW5kaWFAMTIzNA==  # 'mypassword' base64-encoded


secrets-env.yaml
================
apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
    - name: my-container
      image: nginx
      env:
        - name: USERNAME
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: username
        - name: PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: password



secrets-vol.yaml
================
apiVersion: v1
kind: Pod
metadata:
  name: secret-vol-pod
spec:
  containers:
    - name: my-container
      image: nginx
      volumeMounts:
        - name: secret-volume
          mountPath: /etc/secret
          readOnly: true
  volumes:
    - name: secret-volume
      secret:
        secretName: my-secret


commands:
=========
kubectl apply -f secret.yaml
kubectl apply -f secrets-env.yaml
kubectl apply -f secrets-vol.yaml

kubectl get secrets
kubectl describe secret my-secret

kubectl get pods
kubectl exec -it secret-env-pod -- /bin/bash
echo $USERNAME
echo $PASSWORD


kubectl exec -it secret-vol-pod -- /bin/bash
cd /etc/secret
cat username
cat password

kubectl delete -f secret.yaml
kubectl delete -f secrets-env.yaml
kubectl delete -f secrets-vol.yaml
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Namespaces demo
===============
kubectl get namespaces


Creating namespace:
===================
kubectl create namespace dev
kubectl get namespaces

create-ns.yaml
==============
apiVersion: v1
kind: Namespace
metadata:
  name: qa

kubectl apply -f create-ns.yaml

Deleting namespace:
===================
kubectl delete  namespace dev
kubectl get namespaces
kubectl delete -f create-ns.yaml


Switch to the namespace
=======================
kubectl create namespace dev
kubectl create namespace qa
kubectl config set-context --current --namespace=dev
kubectl config view --minify --output 'jsonpath={..namespace}'


kubectl apply -f basic-pod.yaml -n dev
kubectl get pods -n dev
kubectl delete -f basic-pod.yaml -n dev


apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi

apiVersion: v1
kind: ResourceQuota
metadata:
   name: myquota
spec:
  hard:
    limits.cpu: "400m"
    limits.memory: "400Mi"
    requests.cpu: "200m"
    requests.memory: "200Mi"

apiVersion: v1
kind: LimitRange
metadata:
  name: limits
  namespace: qa
spec:
  limits:
    - default:
        cpu: "500m"
        memory: "512Mi"
      defaultRequest:
        cpu: "250m"
        memory: "256Mi"
      type: Container

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Managing Compute Resources for Containers 

resources.yaml
==============
apiVersion: v1
kind: Pod
metadata:
  name: resources
spec:
  containers:
  - name: resource
    image: centos
    command: ["/bin/bash", "-c", "while true; do echo Hello-Kubernetes; sleep 5 ; done"]
    resources:                      # Describes the type of resources to be used
      requests:
        memory: "64Mi"
        cpu: "100m"
      limits:
        memory: "128Mi"
        cpu: "200m"




Commands:
=========
kubectl apply -f resources.yaml
kubectl get pods
kubectl describe pod resources
kubectl delete -f resources.yaml


apiVersion: v1
kind: Pod
metadata:
  name: resources
spec:
  containers:
  - name: resource
    image: centos
    command: ["/bin/bash", "-c", "while true; do echo Hello-Kubernetes; sleep 5 ; done"]
    resources:                      # Describes the type of resources to be used
      requests:
        memory: "6400Mi"
        cpu: "10000m"
      limits:
        memory: "12800Mi"
        cpu: "20000m"

Commands:
=========
kubectl apply -f resources.yaml
kubectl get pods
kubectl describe pod resources
kubectl delete -f resources.yaml

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Resource quota
==============

resourcequota.yaml
==================
apiVersion: v1
kind: ResourceQuota
metadata:
   name: myquota
spec:
  hard:
    limits.cpu: "400m"
    limits.memory: "400Mi"
    requests.cpu: "200m"
    requests.memory: "200Mi"


commands:
=========
kubectl apply -f resourcequota.yaml
kubectl get resourcequota 
kubectl describe resourcequota myquota

rqpod.yaml
==========
kind: Deployment
apiVersion: apps/v1
metadata:
  name: deployments
spec:
  replicas: 3
  selector:      
    matchLabels:
     objtype: deployment
  template:
    metadata:
      name: testpod8
      labels:
        objtype: deployment
    spec:
     containers:
       - name: c00
         image: ubuntu
         command: ["/bin/bash", "-c", "while true; do echo Hello-kubernetes; sleep 5 ; done"]
         resources:
            requests:
              cpu: "200m"
              memory: "128Mi"
            limits:
              cpu: "400m"
              memory: "256Mi"


commands
========
kubectl apply -f rqpod.yaml
kubectl get deployments
kubectl get rs
kubectl get pods
kubectl delete -f rqpod.yaml

Change the replicas to 1 and test (replicas: 3) above manifest file


kubectl apply -f rqpod.yaml
kubectl get deployments
kubectl get rs
kubectl get pods
kubectl delete -f rqpod.yaml

delete the resoucequota

kubectl delete -f resourcequota.yaml
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Limit range
===========

Example:
========
apiVersion: v1
kind: LimitRange
metadata:
  name: limits
  namespace: dev
spec:
  limits:
  - max:
      cpu: "2"
      memory: "1Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    default:
      cpu: "500m"
      memory: "256Mi"
    defaultRequest:
      cpu: "200m"
      memory: "256Mi"
    type: Container


cpulr.yaml
==========

apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 0.5
    defaultRequest:
      cpu: 0.2
    type: Container


commands:
=========
kubectl apply -f cpulr.yaml
kubectl get limitrange
kubectl describe limitrange cpu-limit-range


cpu-limit-pod.yaml
==================
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo-2
spec:
  containers:
  - name: default-cpu-demo-2-ctr
    image: nginx
    resources:
      limits:
        cpu: "0.5"

commands:
=========
kubectl apply -f cpu-limit-pod.yaml
kubectl describe pod default-cpu-demo-2

cpu-request-pod.yaml
====================

apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo-3
spec:
  containers:
  - name: default-cpu-demo-3-ctr
    image: nginx
    resources:
      requests:
        cpu: "0.3"

commands:
=========
kubectl apply -f cpu-request-pod.yaml
kubectl describe pod default-cpu-demo-3
kubectl delete -f cpu-request-pod.yaml


memorylr.yaml
=============
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-min-max-demo-lr
spec:
  limits:
  - max:
      memory: 500Mi
    min:
      memory: 250Mi
    type: Container

commands:
=========
kubectl apply -f memorylr.yaml
kubectl get limitrange
kubectl describe limitrange mem-min-max-demo-lr

memorypod.yaml
==============
apiVersion: v1
kind: Pod
metadata:
  name: constraints-mem-demo
spec:
  containers:
  - name: constraints-mem-demo-ctr
    image: nginx
    resources:
      limits:
        memory: "400Mi"
        #cpu: "0.4"
      requests:
        memory: "300Mi"
        #cpu: "0.3"
commands:
=========
kubectl apply -f memorypod.yaml
kubectl get pods
kubectl describe pod constraints-mem-demo
kubectl delete -f memorypod.yaml



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
*** Change the namespace to default
kubectl config set-context --current --namespace=default
kubectl config view --minify --output 'jsonpath={..namespace}'

cpudeployment.yaml
==================
kind: Deployment
apiVersion: apps/v1
metadata:
   name: mydeploy
spec:
   replicas: 1
   selector:
    matchLabels:
     name: deployment
   template:
     metadata:
       name: testpod8
       labels:
         name: deployment
     spec:
      containers:
        - name: c00
          image: httpd
          ports:
          - containerPort: 80
          resources:
            limits:
              cpu: 500m
            requests:
              cpu: 200m

commands:
=========
kubectl apply -f cpudeployment.yaml
kubectl get pods

update the replicas and verify the application


==========================

HPA-Demo
========
Install metricserver

git clone https://github.com/psddevops/jenkins_pipelines.git

cd jenkins_pipelines

cd metricserver

kubectl apply -f .

kubectl top nodes


hpa-deployment.yaml
===================

apiVersion: apps/v1
kind: Deployment
metadata:
 name: hpa-demo-deployment
spec:
 selector:
   matchLabels:
     run: hpa-demo-deployment
 replicas: 1
 template:
   metadata:
     labels:
       run: hpa-demo-deployment
   spec:
     containers:
     - name: hpa-demo-deployment
       image: k8s.gcr.io/hpa-example
       ports:
       - containerPort: 80
       resources:
         limits:
           cpu: 500m
         requests:
           cpu: 200m


hpa-service.yaml
================
apiVersion: v1
kind: Service
metadata:
 name: hpa-demo-deployment
 labels:
   run: hpa-demo-deployment
spec:
 ports:
 - port: 80
 selector:
   run: hpa-demo-deployment


hpa.yaml
========
---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
 name: hpa-demo-deployment
spec:
 scaleTargetRef:
   apiVersion: apps/v1
   kind: Deployment
   name: hpa-demo-deployment
 minReplicas: 1
 maxReplicas: 10
 targetCPUUtilizationPercentage: 50


Increasing load
===============

kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.02; do wget -q -O- http://hpa-demo-deployment; done"


Commands:
=========
kubectl apply -f hpa-deployment.yaml
kubectl apply -f hpa-service.yaml
kubectl apply -f hpa.yaml


kubectl get deployments
kubectl get rs
kubectl get hpa
kubectl get pods
kubectl describe hpa hpa-demo-deployment


Let's increase the load

kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.02; do wget -q -O- http://hpa-demo-deployment; done"

kubectl delete -f hpa-deployment.yaml
kubectl delete -f hpa-service.yaml
kubectl delete -f hpa.yaml

=========================

Jobs-demo
=========

job1.yaml
=========

apiVersion: batch/v1
kind: Job
metadata:
  name: testjob
spec:
  template:
    metadata:
      name: testjob
    spec:
      containers:
      - name: counter
        image: centos:7
        command: ["/bin/bash", "-c", "echo 'Hi-k8s job'; sleep 5"]
      restartPolicy: Never


commands:
=========
kubectl apply -f job1.yaml
watch kubectl get pods

job2.yaml
=========
apiVersion: batch/v1
kind: Job
metadata:
  name: testjob
spec:
  parallelism: 5                           # Runs for pods in parallel
  activeDeadlineSeconds: 10  # Timesout after 30 sec
  template:
    metadata:
      name: testjob
    spec:
      containers:
      - name: counter
        image: centos:7
        command: ["/bin/bash", "-c", "echo 'Hi-k8s job'; sleep 20"]
      restartPolicy: Never

commands:
=========
kubectl apply -f job2.yaml
watch kubectl get pods

job3.yaml
=========
apiVersion: batch/v1
kind: CronJob
metadata:
 name: ppreddy
spec:
 schedule: "*/2 * * * *"
 jobTemplate:
   spec:
     template:
       spec:
         containers:
         - image: ubuntu
           name: croncont
           command: ["/bin/bash", "-c", "echo 'Hi-k8s job'; sleep 20"]
         restartPolicy: Never

commands:
=========
kubectl apply -f job3.yaml
watch kubectl get pods

==================================

init containers
===============

init-pod-1.yaml
===============
apiVersion: v1
kind: Pod
metadata:
  name: initcontainer
spec:
  initContainers:
  - name: c1
    image: centos
    command: ["/bin/sh", "-c", "echo STAYHOME-STAYSAFE > /tmp/xchange/testfile; sleep 60"]
    volumeMounts:        
      - name: xchange
        mountPath: "/tmp/xchange"  
  containers:
  - name: c2
    image: centos
    command: ["/bin/bash", "-c", "while true; do echo `cat /tmp/data/testfile`; sleep 5; done"]
    volumeMounts:
      - name: xchange
        mountPath: "/tmp/data"
  volumes:                            
  - name: xchange
    emptyDir: {}


Commands:
=========
kubectl apply -f init-pod-1.yaml
kubectl exec initcontainer -it -c c2 -- /bin/bash
kubectl exec initcontainer -it -c c1 -- /bin/bash
kubectl delete -f init-pod-1.yaml



init-pod-2.yaml
===============
apiVersion: v1
kind: Pod
metadata:
  name: initcontainer
spec:
  initContainers:
    - name: ic1
      image: ubuntu
      command: ['/bin/bash', '-c', 'echo Initializing ... ic1... && sleep 60']
    - name: ic2
      image: ubuntu
      command: ['/bin/bash', '-c', 'echo Initializing ... ic2... && sleep 60']
  containers:
    - name: main-init-cont
      image: httpd:latest
      ports:
       - containerPort: 80

Commands:
=========
kubectl apply -f init-pod-2.yaml
kubectl exec initcontainer -it -c main-init-cont -- /bin/bash
kubectl exec initcontainer -it -c ic1 -- /bin/bash
kubectl exec initcontainer -it -c ic2 -- /bin/bash

kubectl delete -f init-pod-1.yaml


==============================================

Recreate demo
=============

build-1.yaml
============
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-1.10
spec:
  replicas: 2
  selector:      
    matchLabels:
      name: nginx
      version: "1.10"
  template:
    metadata:
      labels:
        name: nginx
        version: "1.10"
    spec:
      containers:
        - name: nginx
          image: nginx:1.10
          ports:
            - name: http
              containerPort: 80

build-2.yaml
============
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-1.10
spec:
  replicas: 2
  strategy:
    type: Recreate
  selector:      
    matchLabels:
      name: nginx
      version: "1.10"
  template:
    metadata:
      labels:
        name: nginx
        version: "1.10"
    spec:
      containers:
        - name: nginx
          image: nginx:1.11
          ports:
            - name: http
              containerPort: 80


service.yaml
============
---
  kind: Service                  # Defines to create Service type Object
  apiVersion: v1
  metadata:
    name: democipservice
  spec:
    ports:
      - port: 80                # Containers port exposed
        targetPort: 80          # Pods port
    selector:
        name: nginx
        version: "1.10"         # Apply this service to any pods which has the specific label
    type: ClusterIP             # Specifies the service type i.e ClusterIP or NodePort



Commands:
=========
kubectl apply -f build-1.yaml
kubectl apply -f service.yaml

kubectl describe service/democipservice
curl -s http://10.102.234.176:80/version

curl -s http://10.102.234.176:80/version

kubectl delete -f build-1.yaml
kubectl apply -f service.yaml
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
rolling demo
============

rdbuild-1.yaml
==============
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-1.10
spec:
  replicas: 2
  selector:      
    matchLabels:
      name: nginx
      version: "1.10"
  template:
    metadata:
      labels:
        name: nginx
        version: "1.10"
    spec:
      containers:
        - name: nginx
          image: nginx:1.10
          ports:
            - name: http
              containerPort: 80

rdbuild-2.yaml
==============
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-1.10
spec:
  replicas: 3
  selector:      
    matchLabels:
      name: nginx
      version: "1.10"
  template:
    metadata:
      labels:
        name: nginx
        version: "1.10"
    spec:
      containers:
        - name: nginx
          image: nginx:1.11
          ports:
            - name: http
              containerPort: 80


service.yaml
============
---
  kind: Service                  # Defines to create Service type Object
  apiVersion: v1
  metadata:
    name: democipservice
  spec:
    ports:
      - port: 80                # Containers port exposed
        targetPort: 80          # Pods port
    selector:
        name: nginx
        version: "1.10"         # Apply this service to any pods which has the specific label
    type: ClusterIP             # Specifies the service type i.e ClusterIP or NodePort

Commands:
=========
kubectl apply -f rdbuild-1.yaml
kubectl apply -f service.yaml

kubectl describe service/democipservice
curl -s http://10.100.54.100:80/version

kubectl apply -f rdbuild-2.yaml


kubectl apply -f service.yaml
curl -s http://10.100.54.100:80/version

kubectl delete -f rdbuild-2.yaml
kubectl delete -f service.yaml
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Blue-Green deployment
=====================
blue.yaml
---------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-1.10
spec:
  replicas: 2
  selector:      
    matchLabels:
      name: nginx
      version: "1.10"
  template:
    metadata:
      labels:
        name: nginx
        version: "1.10"
    spec:
      containers:
        - name: nginx
          image: nginx:1.10
          ports:
            - name: http
              containerPort: 80


bgservice.yaml
--------------
---
  kind: Service                  # Defines to create Service type Object
  apiVersion: v1
  metadata:
    name: democipservice
  spec:
    ports:
      - port: 80                # Containers port exposed
        targetPort: 80          # Pods port
    selector:
        name: nginx
        version: "1.10"         # Apply this service to any pods which has the specific label
    type: ClusterIP             # Specifies the service type i.e ClusterIP or NodePort





green.yaml
----------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-1.11
spec:
  replicas: 2
  selector:      
    matchLabels:
      name: nginx
      version: "1.11"
  template:
    metadata:
      labels:
        name: nginx
        version: "1.11"
    spec:
      containers:
        - name: nginx
          image: nginx:1.11
          ports:
            - name: http
              containerPort: 80



Commands:
=========
watch kubectl get all

kubectl apply -f blue.yaml
kubectl apply -f bgservice.yaml
kubectl describe service/democipservice

curl -s http://10.110.71.254/version

kubectl apply -f green.yaml

update "bgservice.yaml" and apply the changes
    selector:
        name: nginx
        version: "1.11"         # Apply this service to any pods which has the specific label
    type: ClusterIP             # Specifies the service type i.e ClusterIP or NodePort


kubectl apply -f bgservice.yaml
curl -s http://10.110.71.254/version

kubectl delete -f bgservice.yaml
kubectl delete -f blue.yaml
kubectl delete -f green.yaml
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Canary deployment Demo
======================


cbuild-1.yaml
=============
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-1.10
spec:
  replicas: 3
  selector:      
    matchLabels:
      name: nginx
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.10
          ports:
            - name: http
              containerPort: 80

cbuild-2.yaml
=============
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-1.11
spec:
  replicas: 1
  selector:      
    matchLabels:
      name: nginx
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.11
          ports:
            - name: http
              containerPort: 80

cservice.yaml
=============
---
  kind: Service                  # Defines to create Service type Object
  apiVersion: v1
  metadata:
    name: democipservice
  spec:
    ports:
      - port: 80                # Containers port exposed
        targetPort: 80          # Pods port
    selector:
        name: nginx             # Apply this service to any pods which has the specific label
    type: ClusterIP             # Specifies the service type i.e ClusterIP or NodePort


Commands:
=========
kubectl apply -f cbuild-1.yaml
kubectl apply -f cservice.yaml

kubectl describe service/democipservice
curl -s http://10.104.61.120/version

kubectl apply -f cbuild-2.yaml
curl -s http://10.104.61.120/version

update the replicas:

cbuild-1.yaml --> replcas:3
cbuild-2.yaml --> replcas:1
curl -s http://10.104.61.120/version

cbuild-1.yaml --> replcas:2
cbuild-2.yaml --> replcas:2
kubectl apply -f cbuild-1.yaml
kubectl apply -f cbuild-2.yaml
curl -s http://10.104.61.120/version

cbuild-1.yaml --> replcas:1
cbuild-2.yaml --> replcas:3
kubectl apply -f cbuild-1.yaml
kubectl apply -f cbuild-2.yaml
curl -s http://10.104.61.120/version


kubectl delete -f cbuild-1.yaml
curl -s http://10.104.61.120/version
kubectl delete -f cbuild-1.yaml
kubectl delete -f cservice.yaml

================================

Node affinity demo
==================

hardna.yaml
===========
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: disktype
                operator: In
                values:
                  - ssd
  containers:
    - name: uc
      image: ubuntu:latest
      command: ["/bin/bash", "-c", "while true; do echo Hello-DevOps; sleep 5; done"]


commands
========
kubectl get nodes --show-labels
kubectl apply -f hardna.yaml
kubectl get pods -o wide
kubectl delete -f hardna.yaml


softna.yaml
===========
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          preference:
            matchExpressions:
              - key: disktype
                operator: In
                values:
                  - ssd1
  containers:
    - name: uc
      image: ubuntu:latest
      command: ["/bin/bash", "-c", "while true; do echo Hello-DevOps; sleep 5; done"]

Commands
========
kubectl apply -f softna.yaml
kubectl get pods -o wide
kubectl delete -f softna.yaml

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Node anti-affinity demo
=======================


hardnaa.yaml
============
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: disktype
                operator: NotIn
                values:
                  - hdd
  containers:
    - name: uc
      image: ubuntu:latest
      command: ["/bin/bash", "-c", "while true; do echo Hello-DevOps; sleep 5; done"]


commands
========
kubectl get nodes --show-labels
kubectl apply -f hardnaa.yaml
kubectl get pods -o wide
kubectl delete -f hardnaa.yaml


softna.yaml
===========
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          preference:
            matchExpressions:
              - key: disktype
                operator: NotIn
                values:
                  - ssd1
  containers:
    - name: uc
      image: ubuntu:latest
      command: ["/bin/bash", "-c", "while true; do echo Hello-DevOps; sleep 5; done"]

Commands
========
kubectl apply -f softnaa.yaml
kubectl get pods -o wide
kubectl delete -f softnaa.yaml

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
pod affinity demo
=================

samplepod.yaml
==============
---
  kind: Pod
  apiVersion: v1
  metadata:
    name: samplepod
    labels:                       # Specifies the Label details under it
      app: psddevops
  spec:
    containers:
      - name: c1
        image: ubuntu:latest
        command: ["/bin/bash", "-c", "while true; do echo Hello-psddevops; sleep 5 ; done"]
    nodeSelector:
      disktype: ssd

Commands
========
kubectl apply -f samplepod.yaml
kubectl get pods -o wide


testpod.yaml
============
---
  kind: Pod
  apiVersion: v1
  metadata:
    name: testpod
    labels:                       # Specifies the Label details under it
      team: dev
  spec:
    containers:
      - name: c1
        image: ubuntu:latest
        command: ["/bin/bash", "-c", "while true; do echo Hello-psddevops; sleep 5 ; done"]
    nodeSelector:
      disktype: hdd

Commands
========
kubectl apply -f testpod.yaml
kubectl get pods -o wide


hardpa.yaml
===========

apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: team
            operator: In  # verify In, NotIn conditions
            values:
            - dev
        topologyKey: "kubernetes.io/hostname"
  containers:
  - name: web-server
    image: nginx


Commands:
=========
kubectl apply -f hardpa.yaml
kubectl get pods -o wide


softpa.yaml
===========
apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  affinity:
    podAffinity:
      # Soft Rule: Prefer to co-locate with frontend pods
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 80
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: NotIn
              values:
              - psddevops
          topologyKey: "kubernetes.io/hostname"
  containers:
  - name: web-server
    image: nginx

Commands:
=========
kubectl apply -f softpa.yaml

=============================================================
pod anti affinity demo
======================
samplepod.yaml
==============
---
  kind: Pod
  apiVersion: v1
  metadata:
    name: samplepod
    labels:                       # Specifies the Label details under it
      app: psddevops
  spec:
    containers:
      - name: c1
        image: ubuntu:latest
        command: ["/bin/bash", "-c", "while true; do echo Hello-psddevops; sleep 5 ; done"]
    nodeSelector:
      disktype: ssd

Commands
========
kubectl apply -f samplepod.yaml
kubectl get pods -o wide


testpod.yaml
============
---
  kind: Pod
  apiVersion: v1
  metadata:
    name: testpod
    labels:                       # Specifies the Label details under it
      team: dev
  spec:
    containers:
      - name: c1
        image: ubuntu:latest
        command: ["/bin/bash", "-c", "while true; do echo Hello-psddevops; sleep 5 ; done"]
    nodeSelector:
      disktype: hdd

Commands
========
kubectl apply -f testpod.yaml
kubectl get pods -o wide


hardpaa.yaml
============

apiVersion: v1
kind: Pod
metadata:
  name: webserverpod
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - psddevops
        topologyKey: "kubernetes.io/hostname"
  containers:
  - name: nginx-cont
    image: nginx


softpaa.yaml
============
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
  labels:
    app: backend
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 50
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - psddevops
          topologyKey: "kubernetes.io/hostname"
  containers:
  - name: nginx
    image: nginx

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Taints and Tolerations
======================

Apply a Taint to Nodes
======================
kubectl taint nodes worker-1 dedicated=logging:NoSchedule
# This taint ensures that no pods, except those with a matching toleration, are scheduled on node1


Apply a Toleration to Pods
To schedule pods on the tainted node, add a toleration in the pod's manifest

ttpod.yaml
==========
apiVersion: v1
kind: Pod
metadata:
  name: logging-pod
spec:
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "logging"
    effect: "NoSchedule"
  containers:
  - name: logging-container
    image: nginx:latest

Commands:
=========
kubectl apply -f ttpod.yaml
kubectl get pods -o wide
kubectl delete -f ttpod.yaml

Key: Matches the taint key on the node.
Operator: Specifies how the toleration is applied (Equal or Exists).
Value: Matches the taint value.
Effect: Must match the taint effect (NoSchedule, PreferNoSchedule, or NoExecute).


kubectl taint nodes worker-2 critical=important:NoExecute

ttpod1.yaml
===========
apiVersion: v1
kind: Pod
metadata:
  name: important-pod
spec:
  tolerations:
  - key: "critical"
    operator: "Equal"
    value: "important"
    effect: "NoExecute"
  containers:
  - name: important-container
    image: nginx

Commands
========
kubectl apply -f ttpod1.yaml
kubectl get pods -o wide
kubectl delete -f ttpod1.yaml


*** Pods without this toleration are immediately evicted from worker-2


Removed the taints

kubectl taint nodes worker-1 dedicated=logging:NoSchedule-
kubectl taint nodes worker-2 critical=important:NoExecute-
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Drain, Cordon and Uncordon
==========================

drain-cordon-uncordon-deployment.yaml
=====================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginxdeploy
spec:
  replicas: 2
  selector:      
    matchLabels:
      name: nginx
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.11
          ports:
            - name: http
              containerPort: 80


Commands:
=========
kubectl apply -f drain-cordon-uncordon-deployment.yaml
kubectl get pods -o wide

kubectl cordon worker-1
kubectl get nodes

kubectl drain worker-1 --ignore-daemonsets --delete-emptydir-data --force
kubectl get pods -o wide

kubectl uncordon worker-1
kubectl get pods -o wide
kubectl delete pod nginxdeploy-598d5c655b-bv6rz
kubectl get pods -o wide
kubectl delete -f drain-cordon-uncordon-deployment.yaml


===============================================================
Static Volume provisioning
==========================


kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-1.22"

kubectl apply -k "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-1.6"


pv.yaml
=======
apiVersion: v1
kind: PersistentVolume
metadata:
  name: efs-pv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumeHandle: fs-012f3739be582185a # Replace with your EFS File System ID


pvc.yaml
========
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc
  resources:
    requests:
      storage: 5Gi


deployment.yaml
===============
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          volumeMounts:
            - name: shared-storage
              mountPath: /usr/share/nginx/html
      volumes:
        - name: shared-storage
          persistentVolumeClaim:
            claimName: efs-pvc

Commands:
=========

kubectl apply -f .

Verify the pod activities

kubectl delete -f .
====================================================

Dynamic volume provisioning
============================

Create a StorageClass

storageclass.yaml
=================
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: efs-sc
provisioner: efs.csi.aws.com
parameters:
  provisioningMode: efs-ap # Automatic creation of access points
  fileSystemId: fs-012f3739be582185a # Replace with your EFS File System ID
  directoryPerms: "777" # Default directory permissions
  gidRangeStart: "1000" # Optional group ID range
  gidRangeEnd: "2000" # Optional group ID range


pvc.yaml
========
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
  storageClassName: efs-sc


deloyment.yaml
==============
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          ports:
            - containerPort: 80
          volumeMounts:
            - name: shared-storage
              mountPath: /usr/share/nginx/html
      volumes:
        - name: shared-storage
          persistentVolumeClaim:
            claimName: efs-pvc


Commands
========
kubectl apply -f .

perform pod activities

kubectl delete -f .

====================================

statefulset-demo
================

nginx-service.yaml
==================
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  clusterIP: None  # Headless service
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 80




storageclass.yaml
=================
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp2
provisioner: kubernetes.io/aws-ebs
volumeBindingMode: WaitForFirstConsumer




statefulset.yaml
=================

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx
spec:
  serviceName: "nginx"
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.21
          ports:
            - containerPort: 80
          volumeMounts:
            - name: nginx-data
              mountPath: /usr/share/nginx/html  # Persisted content directory
  volumeClaimTemplates:
    - metadata:
        name: nginx-data
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "gp2"  # Use the StorageClass created earlier
        resources:
          requests:
            storage: 1Gi


=================================================


RBAC-Demo
=========

kubectl create namespace rbac-test

kubectl get ns

## Deploy nginx pod on clsuster
kubectl create deploy nginx --image=nginx -n rbac-test

## Create IAM user and create access key
aws iam create-user --user-name rbac-user


aws iam create-access-key --user-name rbac-user


Configure the credentials in separate window and login




Create user and run in admin cred

user.yaml
==========
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapUsers: |
    - userarn: arn:aws:iam::533267064682:user/rbac-user
      username: rbac-user


 kubectl apply -f user.yaml (Run with root user)



Verify newly created user able to access or not

PS C:\Users\ppred> kubectl get pods
Error from server (Forbidden): pods is forbidden: User "rbac-user" cannot list resource "pods" in API group "" in the namespace "default"
PS C:\Users\ppred>

 Create role and rolebinding files and run with root 
 ===================================================

role-user.yaml
==============

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: rbac-test
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["list","get","watch"]
- apiGroups: ["extensions","apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch"]



role-binding.yaml
=================
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: rbac-test
subjects:
- kind: User
  name: rbac-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io


PS C:\Users\ppred\demos\rbacdemo> kubectl apply -f .\role-user.yaml
role.rbac.authorization.k8s.io/pod-reader created
PS C:\Users\ppred\demos\rbacdemo> kubectl apply -f .\role-binding.yml
rolebinding.rbac.authorization.k8s.io/read-pods created
PS C:\Users\ppred\demos\rbacdemo>


Login to rbac-user and verify the pods
======================================
PS C:\Users\ppred> kubectl get pods -n rbac-test
NAME                     READY   STATUS    RESTARTS   AGE
nginx-676b6c5bbc-9z5z9   1/1     Running   0          22m
PS C:\Users\ppred>

================================================
EFK-Demo
========

aws ec2 create-tags --resources subnet-0bebce72ecdda4f32 --tags Key=kubernetes.io/role/elb,Value=1 Key=kubernetes.io/cluster/eks_cluster_ohio,Value=owned

aws ec2 create-tags --resources subnet-065f045086e86087d --tags Key=kubernetes.io/role/elb,Value=1 Key=kubernetes.io/cluster/eks_cluster_ohio,Value=owned

aws ec2 create-tags --resources subnet-0e28e70bf0eddd04d --tags Key=kubernetes.io/role/elb,Value=1 Key=kubernetes.io/cluster/eks_cluster_ohio,Value=owned


Create below manifest files
===========================

01_Namespace.yml
================
---
apiVersion: v1
kind: Namespace
metadata:
   name: efklog
...
   

02_ElasticSearch_Service.yaml            
==============================
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-logging
  namespace: efklog
  labels:
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "Elasticsearch"
spec:
  ports:
  - port: 9200
    protocol: TCP
    targetPort: db
  selector:
    k8s-app: elasticsearch-logging



03_ElasticSearch_StatefulSet.yaml
=================================
# RBAC authn and authz
apiVersion: v1
kind: ServiceAccount
metadata:
  name: elasticsearch-logging
  namespace: efklog
  labels:
    k8s-app: elasticsearch-logging
    addonmanager.kubernetes.io/mode: Reconcile
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: elasticsearch-logging
  labels:
    k8s-app: elasticsearch-logging
    addonmanager.kubernetes.io/mode: Reconcile
rules:
- apiGroups:
  - ""
  resources:
  - "services"
  - "namespaces"
  - "endpoints"
  verbs:
  - "get"
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: efklog
  name: elasticsearch-logging
  labels:
    k8s-app: elasticsearch-logging
    addonmanager.kubernetes.io/mode: Reconcile
subjects:
- kind: ServiceAccount
  name: elasticsearch-logging
  namespace: efklog
  apiGroup: ""
roleRef:
  kind: ClusterRole
  name: elasticsearch-logging
  apiGroup: ""
---
# Elasticsearch deployment itself
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch-logging
  namespace: efklog
  labels:
    k8s-app: elasticsearch-logging
    version: v7.3.2
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  serviceName: elasticsearch-logging
  replicas: 2
  selector:
    matchLabels:
      k8s-app: elasticsearch-logging
      version: v7.3.2
  template:
    metadata:
      labels:
        k8s-app: elasticsearch-logging
        version: v7.3.2
    spec:
      serviceAccountName: elasticsearch-logging
      containers:
      - image: quay.io/fluentd_elasticsearch/elasticsearch:v7.3.2
        name: elasticsearch-logging
        imagePullPolicy: Always
        resources:
          # need more cpu upon initialization, therefore burstable class
          limits:
            cpu: 500m
            memory: 3Gi
          requests:
            cpu: 100m
            memory: 1Gi
        ports:
        - containerPort: 9200
          name: db
          protocol: TCP
        - containerPort: 9300
          name: transport
          protocol: TCP
        volumeMounts:
        - name: elasticsearch-logging
          mountPath: /data
        env:
        - name: "NAMESPACE"
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
      volumes:
      - name: elasticsearch-logging
        emptyDir: {}
      # Elasticsearch requires vm.max_map_count to be at least 262144.
      # If your OS already sets up this number to a higher value, feel free
      # to remove this init container.
      initContainers:
      - image: alpine:3.6
        command: ["/sbin/sysctl", "-w", "vm.max_map_count=262144"]
        name: elasticsearch-logging-init
        securityContext:
          privileged: true



04_Fluentd_ConfigMap.yaml
=========================
kind: ConfigMap
apiVersion: v1
metadata:
  name: fluentd-es-config-v0.2.0
  namespace: efklog
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
data:
  system.conf: |-
    <system>
      root_dir /tmp/fluentd-buffers/
    </system>

  containers.input.conf: |-
    # This configuration file for Fluentd / td-agent is used
    # to watch changes to Docker log files. The kubelet creates symlinks that
    # capture the pod name, namespace, container name & Docker container ID
    # to the docker logs for pods in the /var/log/containers directory on the host.
    # If running this fluentd configuration in a Docker container, the /var/log
    # directory should be mounted in the container.
    #
    # These logs are then submitted to Elasticsearch which assumes the
    # installation of the fluent-plugin-elasticsearch & the
    # fluent-plugin-kubernetes_metadata_filter plugins.
    # See https://github.com/uken/fluent-plugin-elasticsearch &
    # https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter for
    # more information about the plugins.
    #
    # Example
    # =======
    # A line in the Docker log file might look like this JSON:
    #
    # {"log":"2014/09/25 21:15:03 Got request with path wombat\n",
    #  "stream":"stderr",
    #   "time":"2014-09-25T21:15:03.499185026Z"}
    #
    # The time_format specification below makes sure we properly
    # parse the time format produced by Docker. This will be
    # submitted to Elasticsearch and should appear like:
    # $ curl 'http://elasticsearch-logging:9200/_search?pretty'
    # ...
    # {
    #      "_index" : "logstash-2014.09.25",
    #      "_type" : "fluentd",
    #      "_id" : "VBrbor2QTuGpsQyTCdfzqA",
    #      "_score" : 1.0,
    #      "_source":{"log":"2014/09/25 22:45:50 Got request with path wombat\n",
    #                 "stream":"stderr","tag":"docker.container.all",
    #                 "@timestamp":"2014-09-25T22:45:50+00:00"}
    #    },
    # ...
    #
    # The Kubernetes fluentd plugin is used to write the Kubernetes metadata to the log
    # record & add labels to the log record if properly configured. This enables users
    # to filter & search logs on any metadata.
    # For example a Docker container's logs might be in the directory:
    #
    #  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b
    #
    # and in the file:
    #
    #  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
    #
    # where 997599971ee6... is the Docker ID of the running container.
    # The Kubernetes kubelet makes a symbolic link to this file on the host machine
    # in the /var/log/containers directory which includes the pod name and the Kubernetes
    # container name:
    #
    #    synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
    #    ->
    #    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
    #
    # The /var/log directory on the host is mapped to the /var/log directory in the container
    # running this instance of Fluentd and we end up collecting the file:
    #
    #   /var/log/containers/synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
    #
    # This results in the tag:
    #
    #  var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
    #
    # The Kubernetes fluentd plugin is used to extract the namespace, pod name & container name
    # which are added to the log message as a kubernetes field object & the Docker container ID
    # is also added under the docker field object.
    # The final tag is:
    #
    #   kubernetes.var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
    #
    # And the final log record look like:
    #
    # {
    #   "log":"2014/09/25 21:15:03 Got request with path wombat\n",
    #   "stream":"stderr",
    #   "time":"2014-09-25T21:15:03.499185026Z",
    #   "kubernetes": {
    #     "namespace": "default",
    #     "pod_name": "synthetic-logger-0.25lps-pod",
    #     "container_name": "synth-lgr"
    #   },
    #   "docker": {
    #     "container_id": "997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b"
    #   }
    # }
    #
    # This makes it easier for users to search for logs by pod name or by
    # the name of the Kubernetes container regardless of how many times the
    # Kubernetes pod has been restarted (resulting in a several Docker container IDs).

    # Json Log Example:
    # {"log":"[info:2016-02-16T16:04:05.930-08:00] Some log text here\n","stream":"stdout","time":"2016-02-17T00:04:05.931087621Z"}
    # CRI Log Example:
    # 2016-02-17T00:04:05.931087621Z stdout F [info:2016-02-16T16:04:05.930-08:00] Some log text here
    <source>
      @id fluentd-containers.log
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/es-containers.log.pos
      tag raw.kubernetes.*
      read_from_head true
      <parse>
        @type multi_format
        <pattern>
          format json
          time_key time
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </pattern>
        <pattern>
          format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
          time_format %Y-%m-%dT%H:%M:%S.%N%:z
        </pattern>
      </parse>
    </source>

    # Detect exceptions in the log output and forward them as one log entry.
    <match raw.kubernetes.**>
      @id raw.kubernetes
      @type detect_exceptions
      remove_tag_prefix raw
      message log
      stream stream
      multiline_flush_interval 5
      max_bytes 500000
      max_lines 1000
    </match>

    # Concatenate multi-line logs
    <filter **>
      @id filter_concat
      @type concat
      key message
      multiline_end_regexp /\n$/
      separator ""
    </filter>

    # Enriches records with Kubernetes metadata
    <filter kubernetes.**>
      @id filter_kubernetes_metadata
      @type kubernetes_metadata
    </filter>

    # Fixes json fields in Elasticsearch
    <filter kubernetes.**>
      @id filter_parser
      @type parser
      key_name log
      reserve_data true
      remove_key_name_field true
      <parse>
        @type multi_format
        <pattern>
          format json
        </pattern>
        <pattern>
          format none
        </pattern>
      </parse>
    </filter>

  system.input.conf: |-
    # Example:
    # 2015-12-21 23:17:22,066 [salt.state       ][INFO    ] Completed state [net.ipv4.ip_forward] at time 23:17:22.066081
    <source>
      @id minion
      @type tail
      format /^(?<time>[^ ]* [^ ,]*)[^\[]*\[[^\]]*\]\[(?<severity>[^ \]]*) *\] (?<message>.*)$/
      time_format %Y-%m-%d %H:%M:%S
      path /var/log/salt/minion
      pos_file /var/log/salt.pos
      tag salt
    </source>

    # Example:
    # Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script
    <source>
      @id startupscript.log
      @type tail
      format syslog
      path /var/log/startupscript.log
      pos_file /var/log/es-startupscript.log.pos
      tag startupscript
    </source>

    # Examples:
    # time="2016-02-04T06:51:03.053580605Z" level=info msg="GET /containers/json"
    # time="2016-02-04T07:53:57.505612354Z" level=error msg="HTTP Error" err="No such image: -f" statusCode=404
    # TODO(random-liu): Remove this after cri container runtime rolls out.
    <source>
      @id docker.log
      @type tail
      format /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
      path /var/log/docker.log
      pos_file /var/log/es-docker.log.pos
      tag docker
    </source>

    # Example:
    # 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal
    <source>
      @id etcd.log
      @type tail
      # Not parsing this, because it doesn't have anything particularly useful to
      # parse out of it (like severities).
      format none
      path /var/log/etcd.log
      pos_file /var/log/es-etcd.log.pos
      tag etcd
    </source>

    # Multi-line parsing is required for all the kube logs because very large log
    # statements, such as those that include entire object bodies, get split into
    # multiple lines by glog.

    # Example:
    # I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537]
    <source>
      @id kubelet.log
      @type tail
      format multiline
      multiline_flush_interval 5s
      format_firstline /^\w\d{4}/
      format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
      time_format %m%d %H:%M:%S.%N
      path /var/log/kubelet.log
      pos_file /var/log/es-kubelet.log.pos
      tag kubelet
    </source>

    # Example:
    # I1118 21:26:53.975789       6 proxier.go:1096] Port "nodePort for kube-system/default-http-backend:http" (:31429/tcp) was open before and is still needed
    <source>
      @id kube-proxy.log
      @type tail
      format multiline
      multiline_flush_interval 5s
      format_firstline /^\w\d{4}/
      format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
      time_format %m%d %H:%M:%S.%N
      path /var/log/kube-proxy.log
      pos_file /var/log/es-kube-proxy.log.pos
      tag kube-proxy
    </source>

    # Example:
    # I0204 07:00:19.604280       5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266]
    <source>
      @id kube-apiserver.log
      @type tail
      format multiline
      multiline_flush_interval 5s
      format_firstline /^\w\d{4}/
      format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
      time_format %m%d %H:%M:%S.%N
      path /var/log/kube-apiserver.log
      pos_file /var/log/es-kube-apiserver.log.pos
      tag kube-apiserver
    </source>

    # Example:
    # I0204 06:55:31.872680       5 servicecontroller.go:277] LB already exists and doesn't need update for service kube-system/kube-ui
    <source>
      @id kube-controller-manager.log
      @type tail
      format multiline
      multiline_flush_interval 5s
      format_firstline /^\w\d{4}/
      format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
      time_format %m%d %H:%M:%S.%N
      path /var/log/kube-controller-manager.log
      pos_file /var/log/es-kube-controller-manager.log.pos
      tag kube-controller-manager
    </source>

    # Example:
    # W0204 06:49:18.239674       7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdatedand cleared (the requested history has been cleared [2578313/2577886]) [2579312]
    <source>
      @id kube-scheduler.log
      @type tail
      format multiline
      multiline_flush_interval 5s
      format_firstline /^\w\d{4}/
      format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
      time_format %m%d %H:%M:%S.%N
      path /var/log/kube-scheduler.log
      pos_file /var/log/es-kube-scheduler.log.pos
      tag kube-scheduler
    </source>

    # Example:
    # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
    <source>
      @id glbc.log
      @type tail
      format multiline
      multiline_flush_interval 5s
      format_firstline /^\w\d{4}/
      format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
      time_format %m%d %H:%M:%S.%N
      path /var/log/glbc.log
      pos_file /var/log/es-glbc.log.pos
      tag glbc
    </source>

    # Example:
    # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
    <source>
      @id cluster-autoscaler.log
      @type tail
      format multiline
      multiline_flush_interval 5s
      format_firstline /^\w\d{4}/
      format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
      time_format %m%d %H:%M:%S.%N
      path /var/log/cluster-autoscaler.log
      pos_file /var/log/es-cluster-autoscaler.log.pos
      tag cluster-autoscaler
    </source>

    # Logs from systemd-journal for interesting services.
    # TODO(random-liu): Remove this after cri container runtime rolls out.
    <source>
      @id journald-docker
      @type systemd
      matches [{ "_SYSTEMD_UNIT": "docker.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-docker.pos
      </storage>
      read_from_head true
      tag docker
    </source>

    <source>
      @id journald-container-runtime
      @type systemd
      matches [{ "_SYSTEMD_UNIT": "{{ fluentd_container_runtime_service }}.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-container-runtime.pos
      </storage>
      read_from_head true
      tag container-runtime
    </source>

    <source>
      @id journald-kubelet
      @type systemd
      matches [{ "_SYSTEMD_UNIT": "kubelet.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-kubelet.pos
      </storage>
      read_from_head true
      tag kubelet
    </source>

    <source>
      @id journald-node-problem-detector
      @type systemd
      matches [{ "_SYSTEMD_UNIT": "node-problem-detector.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-node-problem-detector.pos
      </storage>
      read_from_head true
      tag node-problem-detector
    </source>

    <source>
      @id kernel
      @type systemd
      matches [{ "_TRANSPORT": "kernel" }]
      <storage>
        @type local
        persistent true
        path /var/log/kernel.pos
      </storage>
      <entry>
        fields_strip_underscores true
        fields_lowercase true
      </entry>
      read_from_head true
      tag kernel
    </source>

  forward.input.conf: |-
    # Takes the messages sent over TCP
    <source>
      @id forward
      @type forward
    </source>

  monitoring.conf: |-
    # Prometheus Exporter Plugin
    # input plugin that exports metrics
    <source>
      @id prometheus
      @type prometheus
    </source>

    <source>
      @id monitor_agent
      @type monitor_agent
    </source>

    # input plugin that collects metrics from MonitorAgent
    <source>
      @id prometheus_monitor
      @type prometheus_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>

    # input plugin that collects metrics for output plugin
    <source>
      @id prometheus_output_monitor
      @type prometheus_output_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>

    # input plugin that collects metrics for in_tail plugin
    <source>
      @id prometheus_tail_monitor
      @type prometheus_tail_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>

  output.conf: |-
    <match **>
      @id elasticsearch
      @type elasticsearch
      @log_level info
      type_name _doc
      include_tag_key true
      host elasticsearch-logging
      port 9200
      logstash_format true
      <buffer>
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size 2M
        queue_limit_length 8
        overflow_action block
      </buffer>
    </match>




05_Fluend_DaemonSet.yaml
========================
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd-es
  namespace: efklog
  labels:
    k8s-app: fluentd-es
    addonmanager.kubernetes.io/mode: Reconcile
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluentd-es
  labels:
    k8s-app: fluentd-es
    addonmanager.kubernetes.io/mode: Reconcile
rules:
- apiGroups:
  - ""
  resources:
  - "namespaces"
  - "pods"
  verbs:
  - "get"
  - "watch"
  - "list"
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluentd-es
  labels:
    k8s-app: fluentd-es
    addonmanager.kubernetes.io/mode: Reconcile
subjects:
- kind: ServiceAccount
  name: fluentd-es
  namespace: efklog
  apiGroup: ""
roleRef:
  kind: ClusterRole
  name: fluentd-es
  apiGroup: ""
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-es-v2.7.0
  namespace: efklog
  labels:
    k8s-app: fluentd-es
    version: v2.7.0
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  selector:
    matchLabels:
      k8s-app: fluentd-es
      version: v2.7.0
  template:
    metadata:
      labels:
        k8s-app: fluentd-es
        version: v2.7.0
      # This annotation ensures that fluentd does not get evicted if the node
      # supports critical pod annotation based priority scheme.
      # Note that this does not guarantee admission on the nodes (#40573).
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      serviceAccountName: fluentd-es
      containers:
      - name: fluentd-es
        image: quay.io/fluentd_elasticsearch/fluentd:v2.7.0
        env:
        - name: FLUENTD_ARGS
          value: --no-supervisor -q
        resources:
          limits:
            memory: 500Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config-volume
          mountPath: /etc/fluent/config.d
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config-volume
        configMap:
          name: fluentd-es-config-v0.2.0





06_Kibana_Deployment.yaml
=========================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana-logging
  namespace: efklog
  labels:
    k8s-app: kibana-logging
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: kibana-logging
  template:
    metadata:
      labels:
        k8s-app: kibana-logging
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'
    spec:
      containers:
      - name: kibana-logging
        image: docker.elastic.co/kibana/kibana-oss:7.3.2
        resources:
          # need more cpu upon initialization, therefore burstable class
          limits:
            cpu: 1000m
          requests:
            cpu: 100m
        env:
          - name: ELASTICSEARCH_HOSTS
            value: http://elasticsearch-logging:9200
          - name: SERVER_NAME
            value: kibana-logging
         # - name: SERVER_BASEPATH
         #   value: /api/v1/namespaces/efk-aks/services/kibana-logging/proxy
          - name: SERVER_REWRITEBASEPATH
            value: "false"
        ports:
        - containerPort: 5601
          name: ui
          protocol: TCP



07_Kibana_Service.yaml
======================
apiVersion: v1
kind: Service
metadata:
  name: kibana-logging
  namespace: efklog
  labels:
    k8s-app: kibana-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "Kibana"
spec:
  ports:
  - port: 5601
    protocol: TCP
    targetPort: ui
  selector:
    k8s-app: kibana-logging
  type: LoadBalancer




Access loadbalancer with port no:5601
f2267fd56bd24277a6fa66028263250-1798859452.us-east-2.elb.amazonaws.com:5601/








==================================================

Accessing cluster:
==================
aws eks --region us-east-2 update-kubeconfig --name aws_eks_cluster


Assigning tags to the subnets
=============================
aws ec2 create-tags --resources subnet-0bebce72ecdda4f32 --tags Key=kubernetes.io/role/elb,Value=1 Key=kubernetes.io/cluster/eks_cluster_ohio,Value=owned

aws ec2 create-tags --resources subnet-065f045086e86087d --tags Key=kubernetes.io/role/elb,Value=1 Key=kubernetes.io/cluster/eks_cluster_ohio,Value=owned

aws ec2 create-tags --resources subnet-0e28e70bf0eddd04d --tags Key=kubernetes.io/role/elb,Value=1 Key=kubernetes.io/cluster/eks_cluster_ohio,Value=owned


Installing EBS and EFS with CSI driver
======================================
kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-1.22"

kubectl apply -k "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-1.6"

======================================================