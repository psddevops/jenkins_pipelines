Basic-Example
=============
---
  kind: Pod                          # Object Type
  apiVersion: v1                     # API version
  metadata:                          # Set of data which describes the Object
    name: samplepod                  # Name of the Object
  spec:                              # Data which describes the state of the Object
    containers:                      # Data which describes the Container details
      - name: samplecont             # Name of the Container
        image: ubuntu:latest         # Base Image which is used to create Container
        command: ["/bin/bash", "-c", "while true; do echo Hello-Kubernetes; sleep 5 ; done"]
    restartPolicy: Never             # Defaults to Always


command
=======
kubectl apply -f testpod.yaml
kubectl get pods
kubectl get pods -o wide
kubectl describe pod samplepod
kubectl exec samplepod -it /bin/bash
kubectl exec samplepod -it -c samplecont /bin/bash
kubectl exec samplepod -- hostname -i
kubectl exec samplepod date
kubectl exec samplepod -c samplecont date
kubectl exec samplepod -c samplecont ls
kubectl logs -f samplepod
kubectl logs -f samplepod -c samplecont
kubectl delete -f testpod.yaml



=======================

---
  kind: Pod
  apiVersion: v1
  metadata:
    name: environments
  spec:
    containers:
      - name: mycont
        image: ubuntu:latest
        command: ["/bin/bash", "-c", "while true; do echo Hello-kubernetes; sleep 5 ; done"]
        env:                        # List of environment variables to be used inside the pod
          - name: MYNAME
            value: PPNREDDY

commands
========
kubectl exec environments -- /bin/bash -c  'echo $MYNAME'

==================
---
  kind: Pod
  apiVersion: v1
  metadata:
    name: http-pod
  spec:
    containers:
      - name: mycont
        image: httpd:latest
        ports:
          - containerPort: 80

  Commands
  kubectl exec http-pod -- hostname -i
  curl 10.244.54.7:80
  <html><body><h1>It works!</h1></body></html>


 
=====================

Pods with labels
================

---
  kind: Pod
  apiVersion: v1
  metadata:
    name: labelspod
    labels:                       # Specifies the Label details under it
      env: development
      class: pods
  spec:
    containers:
      - name: labelcont
        image: ubuntu:latest
        command: ["/bin/bash", "-c", "while true; do echo Hello-kubernetes; sleep 5 ; done"]

Commands
========

kubectl get pods --show-labels
kubectl label pods <podname> <labelkey>=<value>
kubectl get pods -l env=development
kubectl get pods -l env=development,class=pods
kubectl delete pod -l <labelkey>=<value>
kubectl get pods -l 'env in (development,testing)'
kubectl get pods -l 'env notin (development,testing)'

===============

Pod with annotations
====================
---
  kind: Pod
  apiVersion: v1
  metadata:
    name: annotations-pod
    labels:
      app: apache
    annotations:
      description: "Deployment for Apache HTTP Server"
      maintainer: "ppreddy@hcltech.com"
  spec:
    containers:
      - name: mycont
        image: httpd:latest
        ports:
          - containerPort: 80


===================

Multi container pods
====================

kind: Pod
apiVersion: v1
metadata:
  name: multicontpod
spec:
  containers:
    - name: container-1
      image: ubuntu:latest
      command: ["/bin/bash", "-c", "while true; do echo Hello-India; sleep 5 ; done"]
    - name: container-2
      image: ubuntu:latest
      command: ["/bin/bash", "-c", "while true; do echo Hello-Karnataka; sleep 5 ; done"]

Commands
========
kubectl get pods
kubectl exec multicontpod -it -c container-1 /bin/bash
kubectl exec multicontpod -it -c container-2 /bin/bash
kubectl logs -f multicontpod -c container-1
kubectl logs -f multicontpod -c container-2

========================

Node Selector
=============
kubectl label nodes ip-10-1-1-18 disktype=ssd

---
  apiVersion: v1
  kind: Pod
  metadata:
    name: my-pod
  spec:
    containers:
      - name: container-1
        image: ubuntu:latest
        command: ["/bin/bash", "-c", "while true; do echo Hello-India; sleep 5 ; done"]
    nodeSelector:
      disktype: ssd

Commands:
kubectl apply -f testpod.yaml
kubectl get pods -o wide
kubectl describe node ip-10-1-1-18

=============================================
---
  kind: ReplicationController         # This defines to create the object of replication type
  apiVersion: v1
  metadata:
    name: myrc
  spec:
    replicas: 2       # This element defines the desired number of pods
    selector: # Tells the controller which pods to watch/belong to this Replication Controller
      myname: ppreddy   # These must match the labels
    template:           # Template element defines a template to launch a new pod
      metadata:
        name: rcpod
        labels:            
          myname: ppreddy
      spec:
        containers:
          - name: rccontainer
            image: ubuntu:latest
            command: ["/bin/bash", "-c", "while true; do echo Hello-ppreddy; sleep 5 ; done"]

Commands
========

kubectl apply -f rc-demo.yaml
kubectl get rc
kubectl get pods
kubectl describe rc myrc
kubectl get pods -o wide
kubectl delete pod myrc-pt9d6
kubectl get pods -o wide

kubectl get pods -l myname=ppreddy
kubectl scale rc myrc --replicas=10
kubectl scale rc myrc --replicas=5

kubectl delete -f rc-demo.yaml


======================================

## ReplicaSet Demo

---
  kind: ReplicaSet                        # Defines the object to be ReplicaSet
  apiVersion: apps/v1                     # Replicaset is not available on v1
  metadata:
    name: myrs
  spec:
    replicas: 2  
    selector:                  
      matchExpressions:                    # These must match the labels
        - {key: myname, operator: In, values: [ppreddy, ppnreddy, preddy]}
        - {key: env, operator: NotIn, values: [production]}
    template:      
      metadata:
        name: rspod
        labels:              
          myname: ppreddy
          env: dev
      spec:
        containers:
          - name: rscontainer
            image: ubuntu:latest
            command: ["/bin/bash", "-c", "while true; do echo Hello-ppreddy; sleep 5 ; done"]



Commands
========
kubectl apply -f rs-demo.yaml
kubectl get rs
kubectl describe rs myrs

kubectl get pods
kubectl delete pod myrs-v7ggf
kubectl get pods

kubectl get pods -l myname=ppreddy
kubectl get pods -l env=dev

kubectl scale rs myrs --replicas=10
kubectl scale rs myrs --replicas=5

kubectl delete -f rs-demo.yaml


=============================================
---
  kind: Deployment
  apiVersion: apps/v1
  metadata:
    name: mydeployments
  spec:
    replicas: 2
    selector:      # tells the controller which pods to watch/belong to
      matchLabels:
        name: deployment
    template:
      metadata:
        name: deppod
        labels:
          name: deployment
      spec:
        containers:
          - name: dep-cont
            image: ubuntu:latest
            command: ["/bin/bash", "-c", "while true; do echo Hello-DevOps; sleep 5; done"]

Commands
========
kubectl apply -f deploy-demo.yaml
kubectl get deployment
kubectl describe deployment mydeployments
kubectl get rs
kubectl get pods
kubectl get pods -o wide

kubectl delete pod  mydeployments-84877d89c9-vw9gd
kubectl get pods

kubectl delete rs mydeployments-84877d89c9
kubectl get rs
kubectl get pods

kubectl scale deployment mydeployments --replicas=10
kubectl get deployment
kubectl describe deployment mydeployments
kubectl get rs
kubectl get pods

kubectl scale deployment mydeployments --replicas=1
kubectl get deployment
kubectl describe deployment mydeployments
kubectl get rs
kubectl get pods

kubectl logs -f mydeployments-84877d89c9-7pk2m

Lets deploy another version
---------------------------
---
  kind: Deployment
  apiVersion: apps/v1
  metadata:
    name: mydeployments
  spec:
    replicas: 2
    selector:      # tells the controller which pods to watch/belong to
      matchLabels:
        name: deployment
    template:
      metadata:
        name: deppod
        labels:
          name: deployment
      spec:
        containers:
          - name: dep-cont
            image: centos:latest
            command: ["/bin/bash", "-c", "while true; do echo Hello-DevOps-centos; sleep 5; done"]


Commands
========
kubectl apply -f deploy-demo.yaml
kubectl get deployment
kubectl describe deployment mydeployments
kubectl get rs
kubectl get pods
kubectl get pods -o wide

kubectl logs -f mydeployments-9cf4bc565-tn9z2

kubectl rollout status deployment mydeployments
kubectl rollout history deployment mydeployments
kubectl rollout undo deploy/mydeployments --to-revision=1

kubectl get rs
kubectl get pods
kubectl get pods -o wide

kubectl logs -f mydeployments-84877d89c9-dwm26
kubectl get deployment
kubectl describe deployment mydeployments

kubectl delete -f deploy-demo.yaml


==========================

# DaemonSet demo

---
  apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    name: nginx-ds
  spec:
    selector:
      matchLabels:
        app: nginx-app
    template:
      metadata:
        name: nginx-pod
        labels:
          app: nginx-app
      spec:
        containers:
          - name: nginx-container
            image: nginx:latest
            ports:
              - containerPort: 80

Commands
========
kubectl apply -f ds-demo.yaml

kubectl get ds
kubectl get rs
kubectl get pods
kubectl get pods -o wide
kubectl describe ds nginx-ds

kubectl delete -f  ds-demo.yaml


===================================
Container to Container communication
====================================

ctoc.yaml
=========

---
  kind: Pod
  apiVersion: v1
  metadata:
    name: testpod
  spec:
    containers:
      - name: uc
        image: ubuntu:latest
        command: ["/bin/bash", "-c", "while true; do echo Hello-Kubernetes; sleep 5 ; done"]
      - name: hc
        image: httpd:latest
        ports:
         - containerPort: 80

commands:
=========
kubectl apply -f ctoc.yaml
kubectl exec testpod -it -c uc -- /bin/bash
root@testpod:/# apt update && apt install curl
root@testpod:/# curl localhost:80
kubectl delete -f ctoc.yaml



============================
Pod to Pod communication
========================

podnginx.yaml
=============

---
  kind: Pod
  apiVersion: v1
  metadata:
    name: testpodnginx
  spec:
    containers:
      - name: nc
        image: nginx:latest
        ports:
         - containerPort: 80


podhttpd.yaml
=============
---
  kind: Pod
  apiVersion: v1
  metadata:
    name: testpodhttpd
  spec:
    containers:
      - name: hc
        image: httpd:latest
        ports:
         - containerPort: 80

commands:
=========
kubectl apply -f podnginx.yaml
kubectl apply -f podhttpd.yaml
kubectl get pods
kubectl get pods -o wide
curl 10.244.54.42:80
curl 10.244.54.41:80
kubectl delete -f podnginx.yaml
kubectl delete -f podhttpd.yaml


=====================================================
clusterIP-Service

clusteripdeployment.yaml
========================

---
  kind: Deployment
  apiVersion: apps/v1
  metadata:
    name: mydeployments
  spec:
    replicas: 2
    selector:      # tells the controller which pods to watch/belong to
      matchLabels:
        name: deployment
    template:
      metadata:
        name: testpod8
        labels:
          name: deployment
      spec:
        containers:
          - name: hc
            image: httpd
            ports:
              - containerPort: 80



clusteripservice.yaml
=====================

---
  kind: Service                  # Defines to create Service type Object
  apiVersion: v1
  metadata:
    name: democipservice
  spec:
    ports:
      - port: 80                # Containers port exposed
        targetPort: 80          # Pods port
    selector:
      name: deployment          # Apply this service to any pods which has the specific label
    type: ClusterIP             # Specifies the service type i.e ClusterIP or NodePort


commands:
=========
kubectl apply -f clusteripdeployment.yaml
kubectl apply -f clusteripservice.yaml
kubectl get pods
kubectl get svc
curl 10.96.78.101:80
kubectl delete pod mydeployments-779f4dbf96-rc89d
kubectl get pods
curl 10.96.78.101:80


================================
NodePort Service
=================


nodeportdeployment.yaml
========================

---
  kind: Deployment
  apiVersion: apps/v1
  metadata:
    name: mydeployments
  spec:
    replicas: 4
    selector:      # tells the controller which pods to watch/belong to
      matchLabels:
        name: deployment
    template:
      metadata:
        name: testpod8
        labels:
          name: deployment
      spec:
        containers:
          - name: hc
            image: httpd
            ports:
              - containerPort: 80



nodeportservice.yaml
====================

---
  kind: Service                  # Defines to create Service type Object
  apiVersion: v1
  metadata:
    name: demonpservice
  spec:
    ports:
      - port: 80                # Containers port exposed
        targetPort: 80          # Pods port
    selector:
      name: deployment          # Apply this service to any pods which has the specific label
    type: NodePort             # Specifies the service type i.e ClusterIP or NodePort


commands:
=========
kubectl apply -f nodeportdeployment.yaml
kubectl apply -f nodeportservice.yaml
kubectl get pods
kubectl get svc

access URL's in browser:
   http://18.218.106.172:30647/
   http://3.145.94.69:30647/
   http://3.129.253.12:30647/

kubectl delete -f nodeportdeployment.yaml
kubectl delete -f nodeportservice.yaml


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

---
  kind: Deployment
  apiVersion: apps/v1
  metadata:
    name: mydeployments
  spec:
    replicas: 1
    selector:      # tells the controller which pods to watch/belong to
      matchLabels:
        name: deployment
    template:
      metadata:
        name: testpod8
        labels:
          name: deployment
      spec:
        containers:
          - name: hc
            image: httpd
            ports:
              - containerPort: 80


apiVersion: v1
kind: Service
metadata:
  name: my-app-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"  # or "clb" for Classic Load Balancer
spec:
  selector:
    app: deployment
  ports:
    - protocol: TCP
      port: 80            # Expose on port 80
      targetPort: 8080     # Target container port
  type: LoadBalancer      # Create an ELB


  ===========================

  Volumes
=======

emptyDir
--------


kubectl apply -f empty-dir-pods.yml
===================================

---
  apiVersion: v1
  kind: Pod
  metadata:
    name: myvolemptydir
  spec:
    containers:
     - name: uc1
       image: ubuntu:latest
       command: ["/bin/bash", "-c", "sleep 10000"]
       volumeMounts:                         # Mount definition inside the container
        - name: xchange
          mountPath: "/tmp/xchange"          # Path inside the container to share
     - name: uc2
       image: ubuntu:latest
       command: ["/bin/bash", "-c", "sleep 10000"]
       volumeMounts:
        - name: xchange
          mountPath: "/tmp/data"
    volumes:                                 # Definition for host
     - name: xchange
       emptyDir: {}


Commands:
=========

kubectl apply -f empty-dir-pods.yml
kubectl exec myvolemptydir -c uc1 -it -- /bin/bash
cd /tmp/xchange
touch uc1.txt

kubectl exec myvolemptydir -c uc2 -it -- /bin/bash
cd /tmp/data/
ls
touch uc2.txt
kubectl delete -f empty-dir-pods.yml



hostPath
--------

hostpath-pods.yml
=================
---
  apiVersion: v1
  kind: Pod
  metadata:
    name: myhostpath
  spec:
    containers:
     - name: uc1
       image: ubuntu:latest
       command: ["/bin/bash", "-c", "sleep 10000"]
       volumeMounts:                         # Mount definition inside the container
        - mountPath: /tmp/hostpath
          name: testvolume
    volumes:                                 # Definition for host
     - name: testvolume
       hostPath:
         path: /tmp/data
         type: DirectoryOrCreate

Commands:
=========
kubectl apply -f hostpath-pods.yml
kubectl get pods -o wide

verify the host where it's created
kubectl delete -f hostpath-pods.yml

=====================


Static provisioning
===================

Create EBS volume in same az and copy the volume id: vol-0c6bcdb099f8bad56

mypv.yaml
---------
apiVersion: v1
kind: PersistentVolume
metadata:
  name: myebsvol
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce 
  persistentVolumeReclaimPolicy: Recycle
  awsElasticBlockStore:
    volumeID: vol-09d341ec00318bab7
    fsType: ext4

#ReadWriteOnce means that the volume can be mounted as read-write by a single node at a time.

#The reclaim policy specifies what happens to the volume when it is released by its associated PersistentVolumeClaim (PVC).

#Recycle means that when the PVC is deleted, the volume will be wiped (its data deleted) and it can be reused by another claim.

#Retain: You want to preserve the data for manual retrieval or reuse.
#Recycle: You want the volume to be cleared of data and available for reuse (though deprecated).
#Delete: You want to completely remove both the volume and the data when no longer needed.

Access Mode Description
=======================
RWO:  ReadWriteOnce Volume can be mounted as read-write by a single node. 
ROX:  ReadOnlyMany  Volume can be mounted as read-only by multiple nodes. 
RWX:  ReadWriteMany Volume can be mounted as read-write by multiple nodes.  
RWOP: ReadWriteOncePod  Volume can be mounted as read-write by a single pod (strictest).



mypvc.yaml
----------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myebsvolclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi





staticvolumepod.yaml
--------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pvdeploy
spec:
  replicas: 1
  selector:      # tells the controller which pods to watch/belong to
    matchLabels:
     app: mypv
  template:
    metadata:
      labels:
        app: mypv
    spec:
      containers:
      - name: shell
        image: centos
        command: ["bin/bash", "-c", "sleep 10000"]
        volumeMounts:
        - name: mypd
          mountPath: "/tmp/persistent"
      volumes:
        - name: mypd
          persistentVolumeClaim:
            claimName: myebsvolclaim



Commands:
=========
kubectl apply -f mypv.yaml
kubectl get pv

kubectl apply -f mypvc.yaml
kubectl get pvc

kubectl apply -f staticvolumepod.yaml


kubectl delete -f mypv.yaml
kubectl delete -f mypvc.yaml
kubectl delete -f staticvolumepod.yaml


===============================

liveness-probe.yaml
===================
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: mylivenessprobe
spec:
  containers:
  - name: liveness
    image: ubuntu
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 1000
    livenessProbe:                             # define the health check
      exec:
        command:                               # command to run periodically
        - cat                
        - /tmp/healthy
      initialDelaySeconds: 5      # Wait for the specified time before it runs the first probe
      periodSeconds: 5            # Run the above command every 5 sec
      timeoutSeconds: 30          # Command must finish within 30 seconds                   

Commands
========
kubectl apply -f liveness-probe.yaml
kubectl exec -it mylivenessprobe -- /bin/bash

cat /tmp/healthy
echo $?
rm /tmp/healthy

kubectl describe pod mylivenessprobe
kubectl delete -f liveness-probe.yaml



===============================
Readinessprobe-demo
==================

rpdeployment.yaml
=================

---
  kind: Deployment
  apiVersion: apps/v1
  metadata:
    name: mydeployments
  spec:
    replicas: 3
    selector:      # tells the controller which pods to watch/belong to
      matchLabels:
        name: deployment
    template:
      metadata:
        name: testpod8
        labels:
          name: deployment
      spec:
        containers:
          - name: hc
            image: httpd
            ports:
              - containerPort: 80
            args:
              - /bin/sh
              - -c
              - touch /tmp/healthy; sleep 1000
            readinessProbe:
              exec:
               command:
                 - cat
                 - /tmp/ready 
              initialDelaySeconds: 15     # Wait 15 seconds before performing the first check
              periodSeconds: 10          # Check every 10 seconds
              timeoutSeconds: 30          # Command must finish within 30 seconds
              successThreshold: 1    # A single successful check marks the container as ready
              failureThreshold: 3    # 3 consecutive failures will mark the pod as not ready


clusteripservice.yaml
=====================

---
  kind: Service                  # Defines to create Service type Object
  apiVersion: v1
  metadata:
    name: democipservice
  spec:
    ports:
      - port: 80                # Containers port exposed
        targetPort: 80          # Pods port
    selector:
      name: deployment          # Apply this service to any pods which has the specific label
    type: ClusterIP             # Specifies the service type i.e ClusterIP or NodePort


commands:
=========
kubectl apply -f clusteripdeployment.yaml
kubectl apply -f clusteripservice.yaml
kubectl get pods
kubectl get svc
curl 10.96.78.101:80
kubectl delete pod mydeployments-779f4dbf96-rc89d
kubectl get pods
curl 10.96.78.101:80

kubectl delete -f clusteripdeployment.yaml
kubectl delete -f clusteripservice.yaml


======================================
config maps demo
================


configmap.yaml
==============

apiVersion: v1
kind: ConfigMap
metadata:
  name: example-config
data:
  # Property-like keys
  config.properties: |
    setting1=psd
    setting2=devops
  # Single key-value pair
  log_level: "INFO"
  # JSON file
  data.json: |
    {
      "key1": "aws",
      "key2": "kubernetes"
    }



configmap-env.yaml
==================

apiVersion: v1
kind: Pod
metadata:
  name: app-pod-env
spec:
  containers:
  - name: app-env-container
    image: nginx
    env:
    - name: LOG_LEVEL
      valueFrom:
        configMapKeyRef:
          name: example-config
          key: log_level


configmap-vol.yaml
==================

apiVersion: v1
kind: Pod
metadata:
  name: app-pod-vol
spec:
  containers:
  - name: app-vol-container
    image: nginx
    volumeMounts:
    - name: config-volume
      mountPath: /etc/config
  volumes:
  - name: config-volume
    configMap:
      name: example-config

commands:
=========
kubectl apply -f configmap.yaml
kubectl apply -f configmap-env.yaml
kubectl apply -f configmap-vol.yaml

kubectl get configmaps
kubectl describe configmap example-config

kubectl exec -it  app-pod-env -- /bin/bash
echo $LOG_LEVEL

kubectl exec -it  app-pod-vol -- /bin/bash
cd /etc/config/

cat config.properties
cat data.json
cat log_level


kubectl delete -f configmap.yaml
kubectl delete -f configmap-env.yaml
kubectl delete -f configmap-vol.yaml


secrets  demo
=============

Create a Secret
===============

kubectl create secret generic my-secret \
  --from-literal=username=ppreddy \
  --from-literal=password=India@123

kubectl get secrets my-secret -o yaml


echo -n 'ppreddy' | base64

echo -n 'India@123' | base64

ubuntu@ip-10-1-1-187:~$ echo -n 'ppreddy' | base64
cHByZWRkeQ==
ubuntu@ip-10-1-1-187:~$ echo -n 'India@1234' | base64
SW5kaWFAMTIzNA==
ubuntu@ip-10-1-1-187:~$


secret.yaml
===========
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  username: cHByZWRkeQ==   # 'myuser' base64-encoded
  password: SW5kaWFAMTIzNA==  # 'mypassword' base64-encoded


secrets-env.yaml
================
apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
    - name: my-container
      image: nginx
      env:
        - name: USERNAME
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: username
        - name: PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: password



secrets-vol.yaml
================
apiVersion: v1
kind: Pod
metadata:
  name: secret-vol-pod
spec:
  containers:
    - name: my-container
      image: nginx
      volumeMounts:
        - name: secret-volume
          mountPath: /etc/secret
          readOnly: true
  volumes:
    - name: secret-volume
      secret:
        secretName: my-secret


commands:
=========
kubectl apply -f secret.yaml
kubectl apply -f secrets-env.yaml
kubectl apply -f secrets-vol.yaml

kubectl get secrets
kubectl describe secret my-secret

kubectl get pods
kubectl exec -it secret-env-pod -- /bin/bash
echo $USERNAME
echo $PASSWORD


kubectl exec -it secret-vol-pod -- /bin/bash
cd /etc/secret
cat username
cat password

kubectl delete -f secret.yaml
kubectl delete -f secrets-env.yaml
kubectl delete -f secrets-vol.yaml
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Namespaces demo
===============
kubectl get namespaces


Creating namespace:
===================
kubectl create namespace dev
kubectl get namespaces

create-ns.yaml
==============
apiVersion: v1
kind: Namespace
metadata:
  name: qa

kubectl apply -f create-ns.yaml

Deleting namespace:
===================
kubectl delete  namespace dev
kubectl get namespaces
kubectl delete -f create-ns.yaml


Switch to the namespace
=======================
kubectl create namespace dev
kubectl create namespace qa
kubectl config set-context --current --namespace=dev
kubectl config view --minify --output 'jsonpath={..namespace}'


kubectl apply -f basic-pod.yaml -n dev
kubectl get pods -n dev
kubectl delete -f basic-pod.yaml -n dev


apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi

apiVersion: v1
kind: ResourceQuota
metadata:
   name: myquota
spec:
  hard:
    limits.cpu: "400m"
    limits.memory: "400Mi"
    requests.cpu: "200m"
    requests.memory: "200Mi"

apiVersion: v1
kind: LimitRange
metadata:
  name: limits
  namespace: qa
spec:
  limits:
    - default:
        cpu: "500m"
        memory: "512Mi"
      defaultRequest:
        cpu: "250m"
        memory: "256Mi"
      type: Container

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Managing Compute Resources for Containers 

resources.yaml
==============
apiVersion: v1
kind: Pod
metadata:
  name: resources
spec:
  containers:
  - name: resource
    image: centos
    command: ["/bin/bash", "-c", "while true; do echo Hello-Kubernetes; sleep 5 ; done"]
    resources:                      # Describes the type of resources to be used
      requests:
        memory: "64Mi"
        cpu: "100m"
      limits:
        memory: "128Mi"
        cpu: "200m"




Commands:
=========
kubectl apply -f resources.yaml
kubectl get pods
kubectl describe pod resources
kubectl delete -f resources.yaml


apiVersion: v1
kind: Pod
metadata:
  name: resources
spec:
  containers:
  - name: resource
    image: centos
    command: ["/bin/bash", "-c", "while true; do echo Hello-Kubernetes; sleep 5 ; done"]
    resources:                      # Describes the type of resources to be used
      requests:
        memory: "6400Mi"
        cpu: "10000m"
      limits:
        memory: "12800Mi"
        cpu: "20000m"

Commands:
=========
kubectl apply -f resources.yaml
kubectl get pods
kubectl describe pod resources
kubectl delete -f resources.yaml

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Resource quota
==============

resourcequota.yaml
==================
apiVersion: v1
kind: ResourceQuota
metadata:
   name: myquota
spec:
  hard:
    limits.cpu: "400m"
    limits.memory: "400Mi"
    requests.cpu: "200m"
    requests.memory: "200Mi"


commands:
=========
kubectl apply -f resourcequota.yaml
kubectl get resourcequota 
kubectl describe resourcequota myquota

rqpod.yaml
==========
kind: Deployment
apiVersion: apps/v1
metadata:
  name: deployments
spec:
  replicas: 3
  selector:      
    matchLabels:
     objtype: deployment
  template:
    metadata:
      name: testpod8
      labels:
        objtype: deployment
    spec:
     containers:
       - name: c00
         image: ubuntu
         command: ["/bin/bash", "-c", "while true; do echo Hello-kubernetes; sleep 5 ; done"]
         resources:
            requests:
              cpu: "200m"
              memory: "128Mi"
            limits:
              cpu: "400m"
              memory: "256Mi"


commands
========
kubectl apply -f rqpod.yaml
kubectl get deployments
kubectl get rs
kubectl get pods
kubectl delete -f rqpod.yaml

Change the replicas to 1 and test (replicas: 3) above manifest file


kubectl apply -f rqpod.yaml
kubectl get deployments
kubectl get rs
kubectl get pods
kubectl delete -f rqpod.yaml

delete the resoucequota

kubectl delete -f resourcequota.yaml
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Limit range
===========

Example:
========
apiVersion: v1
kind: LimitRange
metadata:
  name: limits
  namespace: dev
spec:
  limits:
  - max:
      cpu: "2"
      memory: "1Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    default:
      cpu: "500m"
      memory: "256Mi"
    defaultRequest:
      cpu: "200m"
      memory: "256Mi"
    type: Container


cpulr.yaml
==========

apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 0.5
    defaultRequest:
      cpu: 0.2
    type: Container


commands:
=========
kubectl apply -f cpulr.yaml
kubectl get limitrange
kubectl describe limitrange cpu-limit-range


cpu-limit-pod.yaml
==================
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo-2
spec:
  containers:
  - name: default-cpu-demo-2-ctr
    image: nginx
    resources:
      limits:
        cpu: "0.5"

commands:
=========
kubectl apply -f cpu-limit-pod.yaml
kubectl describe pod default-cpu-demo-2

cpu-request-pod.yaml
====================

apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo-3
spec:
  containers:
  - name: default-cpu-demo-3-ctr
    image: nginx
    resources:
      requests:
        cpu: "0.3"

commands:
=========
kubectl apply -f cpu-request-pod.yaml
kubectl describe pod default-cpu-demo-3
kubectl delete -f cpu-request-pod.yaml


memorylr.yaml
=============
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-min-max-demo-lr
spec:
  limits:
  - max:
      memory: 500Mi
    min:
      memory: 250Mi
    type: Container

commands:
=========
kubectl apply -f memorylr.yaml
kubectl get limitrange
kubectl describe limitrange mem-min-max-demo-lr

memorypod.yaml
==============
apiVersion: v1
kind: Pod
metadata:
  name: constraints-mem-demo
spec:
  containers:
  - name: constraints-mem-demo-ctr
    image: nginx
    resources:
      limits:
        memory: "400Mi"
        #cpu: "0.4"
      requests:
        memory: "300Mi"
        #cpu: "0.3"
commands:
=========
kubectl apply -f memorypod.yaml
kubectl get pods
kubectl describe pod constraints-mem-demo
kubectl delete -f memorypod.yaml



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
*** Change the namespace to default
kubectl config set-context --current --namespace=default
kubectl config view --minify --output 'jsonpath={..namespace}'

cpudeployment.yaml
==================
kind: Deployment
apiVersion: apps/v1
metadata:
   name: mydeploy
spec:
   replicas: 1
   selector:
    matchLabels:
     name: deployment
   template:
     metadata:
       name: testpod8
       labels:
         name: deployment
     spec:
      containers:
        - name: c00
          image: httpd
          ports:
          - containerPort: 80
          resources:
            limits:
              cpu: 500m
            requests:
              cpu: 200m

commands:
=========
kubectl apply -f cpudeployment.yaml
kubectl get pods

update the replicas and verify the application


==========================

HPA-Demo
========
Install metricserver

git clone https://github.com/psddevops/jenkins_pipelines.git

cd jenkins_pipelines

cd metricserver

kubectl apply -f .

kubectl top nodes


hpa-deployment.yaml
===================

apiVersion: apps/v1
kind: Deployment
metadata:
 name: hpa-demo-deployment
spec:
 selector:
   matchLabels:
     run: hpa-demo-deployment
 replicas: 1
 template:
   metadata:
     labels:
       run: hpa-demo-deployment
   spec:
     containers:
     - name: hpa-demo-deployment
       image: k8s.gcr.io/hpa-example
       ports:
       - containerPort: 80
       resources:
         limits:
           cpu: 500m
         requests:
           cpu: 200m


hpa-service.yaml
================
apiVersion: v1
kind: Service
metadata:
 name: hpa-demo-deployment
 labels:
   run: hpa-demo-deployment
spec:
 ports:
 - port: 80
 selector:
   run: hpa-demo-deployment


hpa.yaml
========
---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
 name: hpa-demo-deployment
spec:
 scaleTargetRef:
   apiVersion: apps/v1
   kind: Deployment
   name: hpa-demo-deployment
 minReplicas: 1
 maxReplicas: 10
 targetCPUUtilizationPercentage: 50


Increasing load
===============

kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.02; do wget -q -O- http://hpa-demo-deployment; done"


Commands:
=========
kubectl apply -f hpa-deployment.yaml
kubectl apply -f hpa-service.yaml
kubectl apply -f hpa.yaml


kubectl get deployments
kubectl get rs
kubectl get hpa
kubectl get pods
kubectl describe hpa hpa-demo-deployment


Let's increase the load

kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.02; do wget -q -O- http://hpa-demo-deployment; done"

kubectl delete -f hpa-deployment.yaml
kubectl delete -f hpa-service.yaml
kubectl delete -f hpa.yaml

=========================

Jobs-demo
=========

job1.yaml
=========

apiVersion: batch/v1
kind: Job
metadata:
  name: testjob
spec:
  template:
    metadata:
      name: testjob
    spec:
      containers:
      - name: counter
        image: centos:7
        command: ["/bin/bash", "-c", "echo 'Hi-k8s job'; sleep 5"]
      restartPolicy: Never


commands:
=========
kubectl apply -f job1.yaml
watch kubectl get pods

job2.yaml
=========
apiVersion: batch/v1
kind: Job
metadata:
  name: testjob
spec:
  parallelism: 5                           # Runs for pods in parallel
  activeDeadlineSeconds: 10  # Timesout after 30 sec
  template:
    metadata:
      name: testjob
    spec:
      containers:
      - name: counter
        image: centos:7
        command: ["/bin/bash", "-c", "echo 'Hi-k8s job'; sleep 20"]
      restartPolicy: Never

commands:
=========
kubectl apply -f job2.yaml
watch kubectl get pods

job3.yaml
=========
apiVersion: batch/v1
kind: CronJob
metadata:
 name: ppreddy
spec:
 schedule: "*/2 * * * *"
 jobTemplate:
   spec:
     template:
       spec:
         containers:
         - image: ubuntu
           name: croncont
           command: ["/bin/bash", "-c", "echo 'Hi-k8s job'; sleep 20"]
         restartPolicy: Never

commands:
=========
kubectl apply -f job3.yaml
watch kubectl get pods

==================================

init containers
===============

init-pod-1.yaml
===============
apiVersion: v1
kind: Pod
metadata:
  name: initcontainer
spec:
  initContainers:
  - name: c1
    image: centos
    command: ["/bin/sh", "-c", "echo STAYHOME-STAYSAFE > /tmp/xchange/testfile; sleep 60"]
    volumeMounts:        
      - name: xchange
        mountPath: "/tmp/xchange"  
  containers:
  - name: c2
    image: centos
    command: ["/bin/bash", "-c", "while true; do echo `cat /tmp/data/testfile`; sleep 5; done"]
    volumeMounts:
      - name: xchange
        mountPath: "/tmp/data"
  volumes:                            
  - name: xchange
    emptyDir: {}


Commands:
=========
kubectl apply -f init-pod-1.yaml
kubectl exec initcontainer -it -c c2 -- /bin/bash
kubectl exec initcontainer -it -c c1 -- /bin/bash
kubectl delete -f init-pod-1.yaml



init-pod-2.yaml
===============
apiVersion: v1
kind: Pod
metadata:
  name: initcontainer
spec:
  initContainers:
    - name: ic1
      image: ubuntu
      command: ['/bin/bash', '-c', 'echo Initializing ... ic1... && sleep 60']
    - name: ic2
      image: ubuntu
      command: ['/bin/bash', '-c', 'echo Initializing ... ic2... && sleep 60']
  containers:
    - name: main-init-cont
      image: httpd:latest
      ports:
       - containerPort: 80

Commands:
=========
kubectl apply -f init-pod-2.yaml
kubectl exec initcontainer -it -c main-init-cont -- /bin/bash
kubectl exec initcontainer -it -c ic1 -- /bin/bash
kubectl exec initcontainer -it -c ic2 -- /bin/bash

kubectl delete -f init-pod-1.yaml